"""
XMP å¤šæ¸ é“æ•°æ®å®šæ—¶æŠ“å–è„šæœ¬
æ”¯æŒ TikTok å’Œ Meta (Facebook) æ¸ é“

åŠŸèƒ½:
- å®šæ—¶æŠ“å– campaign ç»´åº¦æ•°æ®
- æ”¯æŒå¤šæ¸ é“ (tiktok, facebook)
- æ•°æ®å­˜å‚¨åˆ° BigQuery
- å¤±è´¥å‘Šè­¦é€šçŸ¥
"""

import os
import sys
import json
import re
import asyncio
import aiohttp
import requests
from datetime import datetime, timedelta, timezone
from typing import Optional, Dict, Any, List

# åŒ—äº¬æ—¶åŒº (UTC+8)
BEIJING_TZ = timezone(timedelta(hours=8))


def get_last_week_workdays(end_date: datetime) -> List[str]:
    """
    è·å–ä¸Šå‘¨æˆ–æœ¬å‘¨çš„å®Œæ•´å·¥ä½œå‘¨ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰

    è§„åˆ™:
    - å‘¨ä¸€åˆ°å‘¨äº”è¿è¡Œ: ç»Ÿè®¡ä¸Šå‘¨çš„å‘¨ä¸€åˆ°å‘¨äº”
    - å‘¨å…­åˆ°å‘¨æ—¥è¿è¡Œ: ç»Ÿè®¡æœ¬å‘¨çš„å‘¨ä¸€åˆ°å‘¨äº”

    Args:
        end_date: å‚è€ƒæ—¥æœŸï¼ˆé€šå¸¸æ˜¯å½“å‰æ—¥æœŸï¼‰

    Returns:
        å·¥ä½œæ—¥åˆ—è¡¨ï¼Œæ ¼å¼ ['YYYY-MM-DD', ...]ï¼ŒæŒ‰æ—¶é—´å‡åº
    """
    current_weekday = end_date.weekday()  # 0=Monday, 6=Sunday

    # åˆ¤æ–­æ˜¯å‘¨æœ«è¿˜æ˜¯å·¥ä½œæ—¥
    if current_weekday >= 5:  # å‘¨å…­(5)æˆ–å‘¨æ—¥(6)
        # å‘¨æœ«: ç»Ÿè®¡æœ¬å‘¨çš„å‘¨ä¸€åˆ°å‘¨äº”
        days_to_this_monday = current_weekday
        target_monday = end_date - timedelta(days=days_to_this_monday)
    else:
        # å·¥ä½œæ—¥: ç»Ÿè®¡ä¸Šå‘¨çš„å‘¨ä¸€åˆ°å‘¨äº”
        days_to_last_monday = current_weekday + 7
        target_monday = end_date - timedelta(days=days_to_last_monday)

    # ç”Ÿæˆå‘¨ä¸€åˆ°å‘¨äº”çš„æ—¥æœŸåˆ—è¡¨
    workdays = []
    for i in range(5):  # å‘¨ä¸€åˆ°å‘¨äº”ï¼Œå…±5å¤©
        day = target_monday + timedelta(days=i)
        workdays.append(day.strftime('%Y-%m-%d'))

    return workdays


# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

# ============ é…ç½® ============
LARK_ALERT_WEBHOOK = os.getenv('LARK_ALERT_WEBHOOK') or os.getenv('LARK_WEBHOOK_URL')
XMP_USERNAME = os.getenv('XMP_USERNAME')
XMP_PASSWORD = os.getenv('XMP_PASSWORD')
XMP_COOKIES_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'xmp_cookies.json')
XMP_TOKEN_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'xmp_token.json')

TOKEN_VALID_DAYS = 15
TOKEN_REFRESH_BEFORE_DAYS = 3

# ============ æŠ•æ‰‹/å‰ªè¾‘å¸ˆåå•é…ç½® ============
# æŠ•æ‰‹åå• (è‹±æ–‡åç”¨äºåŒ¹é… campaign_name ä¸­çš„ optimizer-xxx)
# æ³¨: lyla, juria, jade æ˜¯éŸ©å›½äººä¸ç»Ÿè®¡; eason æ˜¯å‰ªè¾‘å¸ˆä¸æ˜¯æŠ•æ‰‹
OPTIMIZER_LIST = [
    "echo", "felix", "hannibal", "kimi", "kino", "silas", "zane"
]

# å‰ªè¾‘å¸ˆåå• (ä¸­æ–‡å -> å¯èƒ½çš„åˆ«å/è‹±æ–‡å/å§“æ°)
EDITOR_NAME_MAP = {
    "è°¢å¥•ä¿Š": ["eason", "è°¢"],
    "æ¨Šå‡¯ç¿±": ["kyrie", "æ¨Š"],
    "å´æ³½é‘«": ["beita", "å´"],
    "å®‹æ¶µå¦": ["helen", "å®‹"],
    "è‚ä½³æ¬¢": ["maggie", "è‚"],
    "è®¸ä¸¹æ™¨": ["dancey", "è®¸"],
    "ææ–‡æ”¿": ["curry", "æ"],
    "é‚“ç®": ["dorris", "é‚“"],
    "ç‹ä¿Šå–œ": ["ethan", "ç‹"],
    "é™¶ä½³å‡": ["lynn", "é™¶"],
}

# åå‘æ˜ å°„: æ‰€æœ‰å¯èƒ½çš„åå­— -> æ ‡å‡†ä¸­æ–‡å
EDITOR_ALIAS_MAP = {}
for cn_name, aliases in EDITOR_NAME_MAP.items():
    EDITOR_ALIAS_MAP[cn_name] = cn_name
    for alias in aliases:
        EDITOR_ALIAS_MAP[alias] = cn_name
        EDITOR_ALIAS_MAP[alias.lower()] = cn_name


def extract_editor_from_material_name(material_name: str) -> Optional[str]:
    """
    ä»ç´ æåç§°ä¸­æå–å‰ªè¾‘å¸ˆå

    ç´ æåç§°æ ¼å¼: æ—¥æœŸ-å‰ªè¾‘å¸ˆå-å‰§å-è¯­è¨€-åºå·.mp4
    ä¾‹å¦‚:
    - 1.4-è‚ä½³æ¬¢-Eldest Daughter's Marriage Life-ko-12.mp4
    - 12.25-æ¨Š-Eldest Daughter's Marriage Life-ko-4.mp4
    - 12æœˆ1æ—¥-å®‹æ¶µå¦-xxx-5.mp4
    """
    if not material_name:
        return None

    # å°è¯•ç”¨ - åˆ†å‰²ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯å‰ªè¾‘å¸ˆå
    parts = material_name.split('-')
    if len(parts) >= 2:
        editor_part = parts[1].strip()
        # åŒ¹é…å®Œæ•´ä¸­æ–‡å
        if editor_part in EDITOR_ALIAS_MAP:
            return EDITOR_ALIAS_MAP[editor_part]
        # åŒ¹é…å•å§“
        for cn_name, aliases in EDITOR_NAME_MAP.items():
            for alias in aliases:
                if alias == editor_part or alias.lower() == editor_part.lower():
                    return cn_name

    return None


def safe_float(val) -> float:
    """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸º floatï¼Œå¤„ç† Noneã€ç©ºå­—ç¬¦ä¸²ã€'-' ç­‰æƒ…å†µ"""
    if val is None or val == '' or val == '-':
        return 0.0
    try:
        return float(val)
    except (ValueError, TypeError):
        return 0.0


def safe_int(val) -> int:
    """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸º int"""
    if val is None or val == '' or val == '-':
        return 0
    try:
        return int(float(val))
    except (ValueError, TypeError):
        return 0


def send_lark_alert(title: str, content: str, level: str = "warning"):
    """å‘é€é£ä¹¦å‘Šè­¦é€šçŸ¥"""
    if not LARK_ALERT_WEBHOOK:
        print(f"[å‘Šè­¦] æœªé…ç½®é£ä¹¦ Webhook: {title}")
        return False
    level_config = {
        "info": {"color": "blue", "icon": "â„¹ï¸"},
        "warning": {"color": "orange", "icon": "âš ï¸"},
        "error": {"color": "red", "icon": "ğŸš¨"},
    }
    config = level_config.get(level, level_config["warning"])
    msg = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"{config['icon']} {title}"}, "template": config["color"]},
            "elements": [
                {"tag": "div", "text": {"tag": "lark_md", "content": content}},
                {"tag": "div", "text": {"tag": "lark_md", "content": f"**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"}}
            ]
        }
    }
    try:
        resp = requests.post(LARK_ALERT_WEBHOOK, json=msg, timeout=10)
        return resp.status_code == 200
    except:
        return False


class XMPBaseScraper:
    """XMP åŸºç¡€æŠ“å–å™¨ (Token ç®¡ç†)"""

    def __init__(self):
        self.bearer_token = None
        self.token_updated_at = None
        self._load_token()

    def _load_token(self) -> bool:
        if not os.path.exists(XMP_TOKEN_FILE):
            return False
        try:
            with open(XMP_TOKEN_FILE, 'r') as f:
                data = json.load(f)
            self.bearer_token = data.get('token')
            updated_str = data.get('updated')
            if updated_str:
                self.token_updated_at = datetime.fromisoformat(updated_str)
            if self.bearer_token and not self._should_refresh_token():
                print(f"[XMP] å·²åŠ è½½ä¿å­˜çš„ Token")
                return True
        except Exception as e:
            print(f"[XMP] åŠ è½½ Token å¤±è´¥: {e}")
        return False

    def _should_refresh_token(self) -> bool:
        if not self.token_updated_at:
            return True
        days_since_update = (datetime.now() - self.token_updated_at).days
        refresh_threshold = TOKEN_VALID_DAYS - TOKEN_REFRESH_BEFORE_DAYS
        if days_since_update >= refresh_threshold:
            print(f"[XMP] Token å·²ä½¿ç”¨ {days_since_update} å¤©ï¼Œéœ€è¦åˆ·æ–°")
            return True
        print(f"[XMP] Token å·²ä½¿ç”¨ {days_since_update} å¤©ï¼Œæœ‰æ•ˆæœŸå†…")
        return False

    def _save_token(self, token: str):
        try:
            with open(XMP_TOKEN_FILE, 'w') as f:
                json.dump({'token': token, 'updated': datetime.now().isoformat()}, f)
            self.token_updated_at = datetime.now()
            print(f"[XMP] Token å·²ä¿å­˜")
        except Exception as e:
            print(f"[XMP] ä¿å­˜ Token å¤±è´¥: {e}")

    async def login_and_get_token(self, headless: bool = True) -> Optional[str]:
        """ç™»å½• XMP è·å– Token"""
        from playwright.async_api import async_playwright
        print("[XMP] å¯åŠ¨æµè§ˆå™¨ç™»å½•...")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            context = await browser.new_context(viewport={'width': 1920, 'height': 1080})
            if os.path.exists(XMP_COOKIES_FILE):
                try:
                    with open(XMP_COOKIES_FILE, 'r') as f:
                        await context.add_cookies(json.load(f))
                except:
                    pass
            page = await context.new_page()
            captured_token = None

            async def capture_request(request):
                nonlocal captured_token
                auth = request.headers.get('authorization', '')
                if auth.startswith('Bearer ') and not captured_token:
                    captured_token = auth
                    print(f"[XMP] æ•è·åˆ° Token")

            page.on('request', capture_request)
            try:
                await page.goto("https://xmp.mobvista.com/ads_manage/tiktok/account", wait_until='domcontentloaded', timeout=60000)
            except:
                pass
            await asyncio.sleep(3)

            if 'login' in page.url.lower():
                print("[XMP] éœ€è¦ç™»å½•...")
                try:
                    await page.wait_for_selector('input[type="password"]', timeout=10000)
                except:
                    pass
                await page.locator('input[type="text"]').first.fill(XMP_USERNAME)
                await page.locator('input[type="password"]').first.fill(XMP_PASSWORD)
                try:
                    await page.click('button:has-text("ç™»å½•")', timeout=5000)
                except:
                    await page.locator('input[type="password"]').first.press('Enter')
                await asyncio.sleep(5)
                await page.goto("https://xmp.mobvista.com/ads_manage/tiktok/account", wait_until='domcontentloaded', timeout=60000)

            await asyncio.sleep(8)
            if not captured_token:
                await page.reload(wait_until='domcontentloaded')
                await asyncio.sleep(5)

            cookies = await context.cookies()
            with open(XMP_COOKIES_FILE, 'w') as f:
                json.dump(cookies, f)

            if captured_token:
                self._save_token(captured_token)
                self.bearer_token = captured_token

            await browser.close()
            return captured_token

# æ”¯æŒçš„æ¸ é“åŠå…¶æ”¶å…¥å­—æ®µ
CHANNEL_CONFIG = {
    'tiktok': {
        'revenue_fields': ['total_complete_payment_rate'],  # TKæ”¶å…¥ = æ”¯ä»˜å®Œæˆæ€»ä»·å€¼
        'name': 'TikTok'
    },
    'facebook': {
        'revenue_fields': ['purchase_value'],  # Metaæ”¶å…¥ = è´­ç‰©è½¬åŒ–ä»·å€¼
        'name': 'Meta/Facebook'
    }
}

SUPPORTED_CHANNELS = list(CHANNEL_CONFIG.keys())

# API ç«¯ç‚¹
XMP_LIST_URL = "https://xmp-api.mobvista.com/admanage/channel/list"
XMP_SUMMARY_URL = "https://xmp-api.mobvista.com/admanage/channel/summary"
XMP_MATERIAL_LIST_URL = "https://xmp-api.mobvista.com/mediacenter/material/list"

# XMP Open API ç«¯ç‚¹ (ç´ æåº“/ç´ ææŠ¥è¡¨)
XMP_OPEN_MATERIAL_LIST_URL = "https://xmp-open.mobvista.com/v2/media/material/list"
XMP_OPEN_MATERIAL_REPORT_URL = "https://xmp-open.mobvista.com/v2/media/material_report/list"


class XMPMultiChannelScraper(XMPBaseScraper):
    """XMP å¤šæ¸ é“æ•°æ®æŠ“å–å™¨"""

    async def fetch_channel_campaigns(
        self,
        channel: str,
        start_date: str = None,
        end_date: str = None,
        page_size: int = 100
    ) -> Optional[List[Dict[str, Any]]]:
        """
        è·å–æŒ‡å®šæ¸ é“çš„ campaign æ˜ç»†æ•°æ®

        Args:
            channel: æ¸ é“å (tiktok/facebook)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            page_size: æ¯é¡µæ•°é‡

        Returns:
            campaign åˆ—è¡¨
        """
        if channel not in SUPPORTED_CHANNELS:
            print(f"[XMP] ä¸æ”¯æŒçš„æ¸ é“: {channel}")
            return None

        # æ£€æŸ¥ Token
        if not self.bearer_token or self._should_refresh_token():
            print("[XMP] éœ€è¦è·å–/åˆ·æ–° Token...")
            await self.login_and_get_token()

        if not self.bearer_token:
            send_lark_alert("XMP ç™»å½•å¤±è´¥", f"æ— æ³•è·å– {channel} æ•°æ®", level="error")
            return None

        if not start_date:
            start_date = datetime.now().strftime('%Y-%m-%d')
        if not end_date:
            end_date = start_date

        print(f"[XMP] æ‹‰å– {channel} campaign æ˜ç»†: {start_date} ~ {end_date}")

        # è·å–æ¸ é“é…ç½®
        channel_cfg = CHANNEL_CONFIG.get(channel, {})
        revenue_fields = channel_cfg.get('revenue_fields', ['purchase_value'])

        all_campaigns = []
        page = 1

        headers = {
            "Authorization": self.bearer_token,
            "Content-Type": "application/json",
            "Origin": "https://xmp.mobvista.com",
            "Referer": "https://xmp.mobvista.com/"
        }

        # æ„å»ºå­—æ®µåˆ—è¡¨
        base_fields = "campaign_id,campaign_name,cost,impression,click,status,geo"
        revenue_fields_str = ",".join(revenue_fields)
        field_str = f"{base_fields},{revenue_fields_str}"

        while True:
            # TikTok æŒ‰ geo åˆ†ç»„æ—¶æ”¶å…¥å­—æ®µè¿”å› 0ï¼Œæ‰€ä»¥åªæŒ‰ campaign_id åˆ†ç»„
            group_by = "campaign_id" if channel == "tiktok" else "geo,campaign_id"
            payload = {
                "level": "report",
                "channel": channel,
                "start_time": start_date,
                "end_time": end_date,
                "field": field_str,
                "group_by": group_by,
                "page": page,
                "page_size": page_size
            }

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        XMP_LIST_URL,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        result = await response.json()

                        if result.get('code') != 0:
                            print(f"[XMP] API é”™è¯¯: {result.get('msg')}")
                            break

                        data = result.get('data', {})
                        campaigns = data.get('list', [])

                        if not campaigns:
                            break

                        for c in campaigns:
                            cost = float(c.get('cost', 0))
                            # æ”¶å…¥ = æ‰€æœ‰æ”¶å…¥å­—æ®µä¹‹å’Œ
                            revenue = sum(float(c.get(f, 0)) for f in revenue_fields)
                            # TikTok æ”¶å…¥æ˜ç»†
                            tk_complete = float(c.get('total_complete_payment_rate', 0)) if channel == 'tiktok' else 0
                            tk_purchase = float(c.get('total_purchase_value', 0)) if channel == 'tiktok' else 0
                            all_campaigns.append({
                                'channel': channel,
                                'campaign_id': c.get('campaign_id'),
                                'campaign_name': c.get('campaign_name'),
                                'cost': cost,
                                'revenue': revenue,
                                'tk_complete_payment': tk_complete,
                                'tk_purchase_value': tk_purchase,
                                'roas': revenue / cost if cost > 0 else 0,
                                'impression': int(c.get('impression', 0)),
                                'click': int(c.get('click', 0)),
                                'status': c.get('status', ''),
                                'geo': c.get('geo', ''),
                                'date': start_date,
                            })

                        print(f"  ç¬¬ {page} é¡µ: {len(campaigns)} æ¡")

                        if len(campaigns) < page_size:
                            break

                        page += 1
                        await asyncio.sleep(0.5)

            except Exception as e:
                print(f"[XMP] è¯·æ±‚å¤±è´¥: {e}")
                break

        print(f"[XMP] {channel} å…±è·å– {len(all_campaigns)} ä¸ª campaign")
        return all_campaigns

    async def fetch_channel_ads(
        self,
        channel: str,
        start_date: str = None,
        end_date: str = None,
        page_size: int = 100
    ) -> Optional[List[Dict[str, Any]]]:
        """
        è·å–æŒ‡å®šæ¸ é“çš„å¹¿å‘Šç»´åº¦æ•°æ® (ç”¨äºå‰ªè¾‘å¸ˆåˆ†æ¸ é“ç»Ÿè®¡)

        Args:
            channel: æ¸ é“å (tiktok/facebook)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            page_size: æ¯é¡µæ•°é‡

        Returns:
            å¹¿å‘Šåˆ—è¡¨ï¼ŒåŒ…å« ad_name, cost, drill_channel ç­‰
        """
        if channel not in SUPPORTED_CHANNELS:
            print(f"[XMP] ä¸æ”¯æŒçš„æ¸ é“: {channel}")
            return None

        # æ£€æŸ¥ Token
        if not self.bearer_token or self._should_refresh_token():
            print("[XMP] éœ€è¦è·å–/åˆ·æ–° Token...")
            await self.login_and_get_token()

        if not self.bearer_token:
            send_lark_alert("XMP ç™»å½•å¤±è´¥", f"æ— æ³•è·å– {channel} å¹¿å‘Šæ•°æ®", level="error")
            return None

        if not start_date:
            start_date = datetime.now().strftime('%Y-%m-%d')
        if not end_date:
            end_date = start_date

        print(f"[XMP] æ‹‰å– {channel} å¹¿å‘Šç»´åº¦æ•°æ®: {start_date} ~ {end_date}")

        all_ads = []
        page = 1

        headers = {
            "Authorization": self.bearer_token,
            "Content-Type": "application/json",
            "Origin": "https://xmp.mobvista.com",
            "Referer": "https://xmp.mobvista.com/"
        }

        # è·å–æ¸ é“é…ç½®çš„æ”¶å…¥å­—æ®µ
        channel_cfg = CHANNEL_CONFIG.get(channel, {})
        revenue_fields = channel_cfg.get('revenue_fields', ['purchase_value'])
        revenue_fields_str = ",".join(revenue_fields)

        while True:
            payload = {
                "level": "ad",
                "channel": channel,
                "start_time": start_date,
                "end_time": end_date,
                "field": f"ad_name,ad_id,status,cost,impression,click,cpm,cpc,ctr,conversion,cpi,{revenue_fields_str}",
                "page": page,
                "page_size": page_size,
                "report_timezone": ""
            }

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        XMP_LIST_URL,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        result = await response.json()

                        if result.get('code') != 0:
                            print(f"[XMP] API é”™è¯¯: {result.get('msg')}")
                            break

                        data = result.get('data', {})
                        ads = data.get('list', [])

                        if not ads:
                            break

                        for ad in ads:
                            cost = float(ad.get('cost') or 0)
                            # è®¡ç®—æ”¶å…¥: ç´¯åŠ æ‰€æœ‰æ”¶å…¥å­—æ®µ
                            revenue = 0
                            for rf in revenue_fields:
                                revenue += float(ad.get(rf) or 0)

                            all_ads.append({
                                'channel': channel,
                                'ad_id': ad.get('ad_id'),
                                'ad_name': ad.get('ad_name'),
                                'cost': cost,
                                'revenue': revenue,
                                'impression': int(ad.get('impression') or 0),
                                'click': int(ad.get('click') or 0),
                                'conversion': float(ad.get('conversion') or 0),
                                'cpm': float(ad.get('cpm') or 0),
                                'cpc': float(ad.get('cpc') or 0),
                                'ctr': float(ad.get('ctr') or 0),
                                'status': ad.get('status', ''),
                                'date': start_date,
                            })

                        print(f"  ç¬¬ {page} é¡µ: {len(ads)} æ¡å¹¿å‘Š")

                        if len(ads) < page_size:
                            break

                        page += 1
                        await asyncio.sleep(0.5)

            except Exception as e:
                print(f"[XMP] è¯·æ±‚å¤±è´¥: {e}")
                break

        print(f"[XMP] {channel} å…±è·å– {len(all_ads)} æ¡å¹¿å‘Š")
        return all_ads

    async def fetch_channel_designers(
        self,
        channel: str,
        start_date: str = None,
        end_date: str = None,
        page_size: int = 100,
        is_xmp: str = "0"
    ) -> Optional[List[Dict[str, Any]]]:
        """
        è·å–æŒ‡å®šæ¸ é“çš„å‰ªè¾‘å¸ˆ(designer)ç»´åº¦æ•°æ®

        Args:
            channel: æ¸ é“å (facebook)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            page_size: æ¯é¡µæ•°é‡
            is_xmp: æ˜¯å¦ä¸º XMP å‰ªè¾‘å¸ˆ ("0" æˆ– "1")

        Returns:
            å‰ªè¾‘å¸ˆåˆ—è¡¨ï¼ŒåŒ…å« designer_name, cost, revenue ç­‰
        """
        # æ£€æŸ¥ Token
        if not self.bearer_token or self._should_refresh_token():
            print("[XMP] éœ€è¦è·å–/åˆ·æ–° Token...")
            await self.login_and_get_token()

        if not self.bearer_token:
            send_lark_alert("XMP ç™»å½•å¤±è´¥", f"æ— æ³•è·å– {channel} designer æ•°æ®", level="error")
            return None

        if not start_date:
            start_date = datetime.now().strftime('%Y-%m-%d')
        if not end_date:
            end_date = start_date

        print(f"[XMP] æ‹‰å– {channel} designer ç»´åº¦æ•°æ® (is_xmp={is_xmp}): {start_date} ~ {end_date}")

        all_designers = []
        page = 1

        headers = {
            "Authorization": self.bearer_token,
            "Content-Type": "application/json",
            "Origin": "https://xmp.mobvista.com",
            "Referer": "https://xmp.mobvista.com/"
        }

        # è·å–æ¸ é“é…ç½®çš„æ”¶å…¥å­—æ®µ
        channel_cfg = CHANNEL_CONFIG.get(channel, {})
        revenue_fields = channel_cfg.get('revenue_fields', ['purchase_value'])
        revenue_fields_str = ",".join(revenue_fields)

        while True:
            payload = {
                "level": "designer",
                "channel": channel,
                "start_time": start_date,
                "end_time": end_date,
                "field": f"designer_name,currency_cost,impression,cpm,click,cpc,ctr,conversion_rate,{revenue_fields_str}",
                "page": page,
                "page_size": page_size,
                "report_timezone": "",
                "source": "report",
                "score_by": "avg",
                "search": [{"item": "is_xmp", "val": is_xmp, "op": "EQ"}]
            }

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        XMP_LIST_URL,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        result = await response.json()

                        if result.get('code') != 0:
                            print(f"[XMP] API é”™è¯¯: {result.get('msg')}")
                            break

                        data = result.get('data', {})
                        designers = data.get('list', [])

                        if not designers:
                            break

                        for d in designers:
                            # ä½¿ç”¨ safe_float å¤„ç†å¯èƒ½çš„ '-' å€¼
                            cost = safe_float(d.get('currency_cost')) or safe_float(d.get('cost'))
                            # è®¡ç®—æ”¶å…¥
                            revenue = 0
                            for rf in revenue_fields:
                                revenue += safe_float(d.get(rf))

                            all_designers.append({
                                'channel': channel,
                                'designer_name': d.get('designer_name'),
                                'cost': cost,
                                'revenue': revenue,
                                'impression': safe_int(d.get('impression')),
                                'click': safe_int(d.get('click')),
                                'cpm': safe_float(d.get('cpm')),
                                'cpc': safe_float(d.get('cpc')),
                                'ctr': safe_float(d.get('ctr')),
                                'date': start_date,
                            })

                        print(f"  ç¬¬ {page} é¡µ: {len(designers)} æ¡")

                        if len(designers) < page_size:
                            break

                        page += 1
                        await asyncio.sleep(0.5)

            except Exception as e:
                print(f"[XMP] è¯·æ±‚å¤±è´¥: {e}")
                break

        print(f"[XMP] {channel} å…±è·å– {len(all_designers)} æ¡ designer æ•°æ®")
        return all_designers

    async def fetch_channel_summary(
        self,
        channel: str,
        start_date: str = None,
        end_date: str = None
    ) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ¸ é“çš„æ±‡æ€»æ•°æ®"""
        if not self.bearer_token or self._should_refresh_token():
            await self.login_and_get_token()

        if not self.bearer_token:
            return None

        if not start_date:
            start_date = datetime.now().strftime('%Y-%m-%d')
        if not end_date:
            end_date = start_date

        # è·å–æ¸ é“é…ç½®
        channel_cfg = CHANNEL_CONFIG.get(channel, {})
        revenue_fields = channel_cfg.get('revenue_fields', ['purchase_value'])
        revenue_fields_str = ",".join(revenue_fields)

        payload = {
            "level": "account",
            "channel": channel,
            "start_time": start_date,
            "end_time": end_date,
            "field": f"cost,{revenue_fields_str},impression,click",
            "page": 1,
            "page_size": 100
        }

        headers = {
            "Authorization": self.bearer_token,
            "Content-Type": "application/json",
            "Origin": "https://xmp.mobvista.com",
            "Referer": "https://xmp.mobvista.com/"
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    XMP_SUMMARY_URL,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    result = await response.json()

                    if result.get('code') == 0:
                        sum_data = result.get('data', {}).get('sum', {})
                        cost = float(sum_data.get('cost', 0))
                        # æ”¶å…¥ = æ‰€æœ‰æ”¶å…¥å­—æ®µä¹‹å’Œ
                        revenue = sum(float(sum_data.get(f, 0)) for f in revenue_fields)

                        return {
                            'channel': channel,
                            'date': start_date,
                            'cost': cost,
                            'revenue': revenue,
                            'roas': revenue / cost if cost > 0 else 0,
                            'impression': int(sum_data.get('impression', 0)),
                            'click': int(sum_data.get('click', 0)),
                        }
        except Exception as e:
            print(f"[XMP] {channel} æ±‡æ€»è¯·æ±‚å¤±è´¥: {e}")

        return None

    async def fetch_all_channels(
        self,
        start_date: str = None,
        end_date: str = None
    ) -> Dict[str, Any]:
        """
        æŠ“å–æ‰€æœ‰æ¸ é“æ•°æ®

        Returns:
            {
                'summary': {channel: summary_data},
                'campaigns': [all campaigns],
                'timestamp': datetime
            }
        """
        if not start_date:
            start_date = datetime.now().strftime('%Y-%m-%d')
        if not end_date:
            end_date = start_date

        print(f"\n{'='*50}")
        print(f"[XMP] å¼€å§‹æŠ“å–æ‰€æœ‰æ¸ é“æ•°æ®: {start_date}")
        print(f"{'='*50}")

        result = {
            'summary': {},
            'campaigns': [],
            'timestamp': datetime.now().isoformat(),
            'date': start_date
        }

        for channel in SUPPORTED_CHANNELS:
            print(f"\n--- {channel.upper()} ---")

            # è·å–æ±‡æ€»
            summary = await self.fetch_channel_summary(channel, start_date, end_date)
            if summary:
                result['summary'][channel] = summary
                print(f"  æ±‡æ€»: cost=${summary['cost']:,.2f}, revenue=${summary['revenue']:,.2f}, ROAS={summary['roas']*100:.1f}%")

            # è·å–æ˜ç»†
            campaigns = await self.fetch_channel_campaigns(channel, start_date, end_date)
            if campaigns:
                result['campaigns'].extend(campaigns)

            await asyncio.sleep(1)

        # æ‰“å°æ€»æ±‡æ€»
        total_cost = sum(s['cost'] for s in result['summary'].values())
        total_rev = sum(s['revenue'] for s in result['summary'].values())

        print(f"\n{'='*50}")
        print(f"[XMP] æŠ“å–å®Œæˆ")
        print(f"  æ€» campaign æ•°: {len(result['campaigns'])}")
        print(f"  æ€»æ¶ˆè€—: ${total_cost:,.2f}")
        print(f"  æ€»æ”¶å…¥: ${total_rev:,.2f}")
        print(f"  æ•´ä½“ ROAS: {total_rev/total_cost*100:.1f}%" if total_cost > 0 else "  ROAS: N/A")
        print(f"{'='*50}\n")

        return result


class XMPEditorStatsScraper(XMPBaseScraper):
    """XMP å‰ªè¾‘å¸ˆç»Ÿè®¡æ•°æ®æŠ“å–å™¨ (ä½¿ç”¨ Open API)"""

    def __init__(self):
        super().__init__()
        # Open API é…ç½®
        self.client_id = os.getenv('XMP_CLIENT_ID')
        self.client_secret = os.getenv('XMP_CLIENT_SECRET')

    def _generate_sign(self, timestamp: int) -> str:
        """ç”Ÿæˆç­¾å: sign = md5(client_secret + timestamp)"""
        import hashlib
        sign_str = f"{self.client_secret}{timestamp}"
        return hashlib.md5(sign_str.encode()).hexdigest()

    def _make_request(self, url: str, payload: dict) -> Optional[dict]:
        """å‘é€ API è¯·æ±‚"""
        import time
        headers = {"Content-Type": "application/json"}
        max_retries = 3
        retry_delays = [10, 30, 60]

        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=payload, headers=headers, timeout=60)
                result = response.json()

                if result.get("code") == 0:
                    return result.get("data")

                error_msg = result.get("msg", "Unknown error")
                if "é¢‘ç¹" in error_msg or "too many" in error_msg.lower():
                    if attempt < max_retries - 1:
                        delay = retry_delays[attempt]
                        print(f"[é™é¢‘] {delay}ç§’åé‡è¯•...")
                        time.sleep(delay)
                        continue

                print(f"[APIé”™è¯¯] code={result.get('code')}, msg={error_msg}")
                return None

            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(retry_delays[attempt])
                    continue
                print(f"[é”™è¯¯] {e}")
                return None
        return None

    def fetch_material_details(
        self,
        start_date: str = None,
        end_date: str = None,
        folder_id: List[int] = None,
        user_material_id: List[str] = None,
        md5_file_id: List[str] = None,
        is_deleted: int = 0,
        page_size: int = 500
    ) -> List[dict]:
        """
        è·å–ç´ æè¯¦æƒ…åˆ—è¡¨ (v2 API)

        Args:
            start_date: åˆ›å»ºå¼€å§‹æ—¥æœŸ YYYY-MM-DD
            end_date: åˆ›å»ºç»“æŸæ—¥æœŸ YYYY-MM-DD (æœ€é•¿è·¨åº¦30å¤©)
            folder_id: æ–‡ä»¶å¤¹IDåˆ—è¡¨
            user_material_id: ç´ ælocal idåˆ—è¡¨
            md5_file_id: ç´ æmd5åˆ—è¡¨
            is_deleted: æ˜¯å¦åˆ é™¤ (0æœªåˆ é™¤, 1å·²åˆ é™¤, Noneå…¨éƒ¨)
            page_size: æ¯é¡µæ•°é‡ (1-1000)

        Returns:
            ç´ æè¯¦æƒ…åˆ—è¡¨ï¼ŒåŒ…å«:
            - material_name: ç´ æåç§°
            - designer: è®¾è®¡å¸ˆ/å‰ªè¾‘å¸ˆ
            - shape: å½¢çŠ¶ (ç«–ç‰ˆ/æ¨ªç‰ˆ/æ–¹å½¢)
            - duration: æ—¶é•¿(ç§’)
            - file_url: æ–‡ä»¶åœ°å€
            - folder_name: æ–‡ä»¶å¤¹åç§°
            - created_time: åˆ›å»ºæ—¶é—´
        """
        import time
        if not self.client_id or not self.client_secret:
            print("[XMP] æœªé…ç½® XMP_CLIENT_ID/XMP_CLIENT_SECRET")
            return []

        # è‡³å°‘éœ€è¦ä¸€ä¸ªæŸ¥è¯¢æ¡ä»¶
        if not any([start_date, folder_id, user_material_id, md5_file_id]):
            print("[XMP] é”™è¯¯: è‡³å°‘éœ€è¦ä¼ å…¥ä¸€ä¸ªæŸ¥è¯¢æ¡ä»¶ (start_date/folder_id/user_material_id/md5_file_id)")
            return []

        all_materials = []
        page = 1

        date_range = f"{start_date} ~ {end_date}" if start_date else "æ— æ—¥æœŸé™åˆ¶"
        print(f"[XMP] è·å–ç´ æè¯¦æƒ…: {date_range}")

        while True:
            timestamp = int(time.time())
            payload = {
                "client_id": self.client_id,
                "timestamp": timestamp,
                "sign": self._generate_sign(timestamp),
                "page": page,
                "page_size": page_size
            }

            # æ·»åŠ å¯é€‰å‚æ•°
            if start_date:
                payload["start_date"] = start_date
            if end_date:
                payload["end_date"] = end_date
            if folder_id:
                payload["folder_id"] = folder_id
            if user_material_id:
                payload["user_material_id"] = user_material_id
            if md5_file_id:
                payload["md5_file_id"] = md5_file_id
            if is_deleted is not None:
                payload["is_deleted"] = is_deleted

            data = self._make_request(XMP_OPEN_MATERIAL_LIST_URL, payload)
            if not data:
                break

            # API è¿”å›åˆ—è¡¨æˆ– {list: [...]} ç»“æ„
            if isinstance(data, list):
                materials = data
            else:
                materials = data if isinstance(data, list) else []

            if not materials:
                break

            all_materials.extend(materials)
            print(f"[XMP] ç¬¬ {page} é¡µ: {len(materials)} ä¸ªç´ æï¼Œç´¯è®¡ {len(all_materials)} ä¸ª")

            if len(materials) < page_size:
                break

            page += 1
            time.sleep(0.5)  # QPM é™é¢‘ä¿æŠ¤

        print(f"[XMP] ç´ æè¯¦æƒ…å…± {len(all_materials)} æ¡è®°å½•")
        return all_materials

    def fetch_material_list(self, md5_file_id: List[str] = None) -> List[dict]:
        """è·å–ç´ æåº“åˆ—è¡¨ (å«å‰ªè¾‘å¸ˆä¿¡æ¯)"""
        import time
        if not self.client_id or not self.client_secret:
            print("[XMP] æœªé…ç½® XMP_CLIENT_ID/XMP_CLIENT_SECRET")
            return []

        all_materials = []
        page = 1
        page_size = 100

        print(f"[XMP] è·å–ç´ æåº“åˆ—è¡¨...")

        while True:
            timestamp = int(time.time())
            payload = {
                "client_id": self.client_id,
                "timestamp": timestamp,
                "sign": self._generate_sign(timestamp),
                "page": page,
                "page_size": page_size
            }
            if md5_file_id:
                payload["md5_file_id"] = md5_file_id

            data = self._make_request(XMP_OPEN_MATERIAL_LIST_URL, payload)
            if not data:
                break

            # API å¯èƒ½ç›´æ¥è¿”å›åˆ—è¡¨æˆ– {list: [...]} ç»“æ„
            if isinstance(data, list):
                materials = data
            else:
                materials = data.get("list", [])
            if not materials:
                break

            all_materials.extend(materials)
            print(f"[XMP] ç¬¬ {page} é¡µ: {len(materials)} ä¸ªç´ æ")

            if len(materials) < page_size:
                break
            page += 1
            time.sleep(0.5)

        print(f"[XMP] å…±è·å– {len(all_materials)} ä¸ªç´ æ")
        return all_materials

    def fetch_material_report(
        self,
        start_date: str,
        end_date: str,
        dimension: List[str] = None,
        metrics: List[str] = None
    ) -> List[dict]:
        """
        è·å–ç´ ææŠ¥è¡¨æ•°æ® (å«æ¶ˆè€—å’Œæ”¶å…¥ä¿¡æ¯)

        Args:
            start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD
            dimension: ç»´åº¦åˆ—è¡¨ï¼Œé»˜è®¤ ["md5_file_id", "material_name"]
            metrics: æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤åŒ…å«èŠ±è´¹ã€æ”¶å…¥ç­‰æ ¸å¿ƒæŒ‡æ ‡

        Returns:
            ç´ ææŠ¥è¡¨æ•°æ®åˆ—è¡¨
        """
        import time
        if not self.client_id or not self.client_secret:
            print("[XMP] æœªé…ç½® XMP_CLIENT_ID/XMP_CLIENT_SECRET")
            return []

        all_data = []
        page = 1
        page_size = 500

        # é»˜è®¤ç»´åº¦å’ŒæŒ‡æ ‡
        if dimension is None:
            dimension = ["md5_file_id", "material_name"]
        if metrics is None:
            # æ ¸å¿ƒæŒ‡æ ‡: èŠ±è´¹ã€æ”¶å…¥ã€å±•ç¤ºã€ç‚¹å‡»
            metrics = [
                "currency_cost",      # èŠ±è´¹ (USD)
                "total_purchase_value",  # ä»˜è´¹é‡‘é¢/æ”¶å…¥
                "impression",         # å±•ç¤ºæ•°
                "click",              # ç‚¹å‡»æ•°
            ]

        print(f"[XMP] æ‹‰å–ç´ ææŠ¥è¡¨: {start_date} ~ {end_date}")
        print(f"[XMP] ç»´åº¦: {dimension}, æŒ‡æ ‡: {metrics}")

        while True:
            timestamp = int(time.time())
            payload = {
                "client_id": self.client_id,
                "timestamp": timestamp,
                "sign": self._generate_sign(timestamp),
                "start_date": start_date,
                "end_date": end_date,
                "metrics": metrics,
                "dimension": dimension,
                "page": page,
                "page_size": page_size,
                "cost_currency": "usd"
            }

            data = self._make_request(XMP_OPEN_MATERIAL_REPORT_URL, payload)
            if not data:
                break

            records = data.get("list", [])
            if not records:
                break

            all_data.extend(records)
            print(f"[XMP] ç¬¬ {page} é¡µ: {len(records)} æ¡")

            if len(records) < page_size:
                break
            page += 1
            time.sleep(6)  # QPM é™é¢‘ä¿æŠ¤ (10 QPM)

        print(f"[XMP] ç´ ææŠ¥è¡¨å…± {len(all_data)} æ¡è®°å½•")
        return all_data

    def _extract_designer_name(self, designer_raw) -> str:
        """ä» designer å­—æ®µæå–è®¾è®¡å¸ˆåç§°"""
        if isinstance(designer_raw, dict):
            return designer_raw.get("name") or designer_raw.get("designer_name") or "æœªçŸ¥"
        elif isinstance(designer_raw, list):
            first = designer_raw[0] if designer_raw else None
            if isinstance(first, dict):
                return first.get("name") or first.get("designer_name") or "æœªçŸ¥"
            else:
                return first or "æœªçŸ¥"
        else:
            return designer_raw or "æœªçŸ¥"

    def _extract_optimizer_from_name(self, name: str) -> str:
        """ä» campaign_name ä¸­æå–æŠ•æ‰‹è‹±æ–‡å"""
        if not name:
            return "æœªçŸ¥"
        name_lower = name.lower()
        for optimizer in OPTIMIZER_LIST:
            if optimizer.lower() in name_lower:
                return optimizer.lower()
        return "æœªçŸ¥"

    def _extract_editor_from_name(self, name: str) -> str:
        """ä»å¹¿å‘Šåç§°ä¸­æœç´¢å‰ªè¾‘å¸ˆ (ä¸­è‹±æ–‡å)"""
        if not name:
            return "æœªçŸ¥"
        # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ä¸­æ–‡å
        for cn_name in EDITOR_NAME_MAP.keys():
            if cn_name in name:
                return cn_name
        # å†å°è¯•åŒ¹é…åˆ«å
        name_lower = name.lower()
        for alias, cn_name in EDITOR_ALIAS_MAP.items():
            if alias.lower() in name_lower:
                return cn_name
        return "æœªçŸ¥"

    def fetch_editor_stats(
        self,
        start_date: str,
        end_date: str,
        hot_threshold: float = 500.0,
        roas_threshold: float = 0.45
    ) -> List[dict]:
        """
        è·å–å‰ªè¾‘å¸ˆç»Ÿè®¡æ•°æ® (å‘¨æŠ¥ç”¨)

        ç»Ÿè®¡æŒ‡æ ‡:
        - material_count: ä¸Šæ–°ç´ ææ•°
        - total_cost: æ¶ˆè€—è´¡çŒ®
        - total_revenue: æ€»æ”¶å…¥
        - d0_roas: D0 ROAS (æ”¶å…¥/èŠ±è´¹)
        - hot_count: çˆ†æ¬¾æ•° (>$500 ä¸” ROI è¾¾æ ‡)
        - hot_rate: çˆ†æ¬¾ç‡
        - top_material: Top ç´ æåç§°

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            hot_threshold: çˆ†æ¬¾æ¶ˆè€—é˜ˆå€¼ (é»˜è®¤ $500)
            roas_threshold: çˆ†æ¬¾ ROAS é˜ˆå€¼ (é»˜è®¤ 45%)
        """
        print(f"[XMP] è·å–å‰ªè¾‘å¸ˆç»Ÿè®¡: {start_date} ~ {end_date}")

        # 1. è·å–ç´ ææŠ¥è¡¨ (æ¶ˆè€—+æ”¶å…¥æ•°æ®)
        material_report = self.fetch_material_report(start_date, end_date)
        if not material_report:
            print("[XMP] ç´ ææŠ¥è¡¨ä¸ºç©º")
            return []

        # æ„å»º md5 -> {cost, revenue, name} æ˜ å°„
        md5_data_map = {}
        for item in material_report:
            md5 = item.get("md5_file_id")
            if not md5:
                continue
            cost = float(item.get("currency_cost", 0))
            revenue = float(item.get("total_purchase_value", 0))
            name = item.get("material_name", "")

            if md5 not in md5_data_map:
                md5_data_map[md5] = {"cost": 0, "revenue": 0, "name": name}

            md5_data_map[md5]["cost"] += cost
            md5_data_map[md5]["revenue"] += revenue
            if not md5_data_map[md5]["name"] and name:
                md5_data_map[md5]["name"] = name

        # 2. è·å–ç´ æåº“ (å‰ªè¾‘å¸ˆä¿¡æ¯)
        md5_list = list(md5_data_map.keys())
        materials = self.fetch_material_list(md5_file_id=md5_list)
        if not materials:
            print("[XMP] ç´ æåº“ä¸ºç©º")
            return []

        # æ„å»º md5 -> designer æ˜ å°„
        md5_designer_map = {}
        for mat in materials:
            md5 = mat.get("md5_file_id")
            designer = self._extract_designer_name(mat.get("designer"))
            md5_designer_map[md5] = designer

        # 3. æŒ‰å‰ªè¾‘å¸ˆèšåˆ
        editor_stats = {}
        for md5, data in md5_data_map.items():
            designer = md5_designer_map.get(md5, "æœªçŸ¥")
            cost = data["cost"]
            revenue = data["revenue"]
            name = data["name"]
            roas = revenue / cost if cost > 0 else 0

            if designer not in editor_stats:
                editor_stats[designer] = {
                    "name": designer,
                    "material_count": 0,
                    "total_cost": 0,
                    "total_revenue": 0,
                    "hot_count": 0,
                    "materials": []  # ç”¨äºæ‰¾ Top ç´ æ
                }

            editor_stats[designer]["material_count"] += 1
            editor_stats[designer]["total_cost"] += cost
            editor_stats[designer]["total_revenue"] += revenue

            # çˆ†æ¬¾åˆ¤å®š: æ¶ˆè€— > é˜ˆå€¼ ä¸” ROAS è¾¾æ ‡
            if cost >= hot_threshold and roas >= roas_threshold:
                editor_stats[designer]["hot_count"] += 1

            # è®°å½•ç´ æç”¨äºæ‰¾ Top
            editor_stats[designer]["materials"].append({
                "name": name,
                "md5": md5,
                "cost": cost,
                "revenue": revenue,
                "roas": roas
            })

        # 4. è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        result = []
        for editor in editor_stats.values():
            count = editor["material_count"]
            cost = editor["total_cost"]
            revenue = editor["total_revenue"]

            # æ‰¾ Top ç´ æ (æŒ‰æ¶ˆè€—æ’åº)
            materials_sorted = sorted(
                editor["materials"],
                key=lambda x: x["cost"],
                reverse=True
            )
            top_mat = materials_sorted[0] if materials_sorted else None

            result.append({
                "name": editor["name"],
                "material_count": count,
                "total_cost": cost,
                "total_revenue": revenue,
                "d0_roas": revenue / cost if cost > 0 else 0,
                "hot_count": editor["hot_count"],
                "hot_rate": editor["hot_count"] / count if count > 0 else 0,
                "top_material": top_mat["name"] if top_mat else "",
                "top_material_cost": top_mat["cost"] if top_mat else 0,
                "top_material_roas": top_mat["roas"] if top_mat else 0,
            })

        # æŒ‰æ¶ˆè€—æ’åº
        result.sort(key=lambda x: x["total_cost"], reverse=True)

        print(f"[XMP] å‰ªè¾‘å¸ˆç»Ÿè®¡: {len(result)} äºº")
        return result

    def fetch_editor_performance(
        self,
        start_date: str,
        end_date: str,
        hot_threshold: float = 500.0,
        roas_threshold: float = 0.45
    ) -> Dict[str, Any]:
        """
        è·å–å‰ªè¾‘å¸ˆäº§å‡ºä¸è´¨é‡æŠ¥è¡¨ (Editor Performance)

        å¯¹åº”å›¾è¡¨å­—æ®µ:
        - å‰ªè¾‘å¸ˆ: name
        - ä¸Šæ–°ç´ ææ•°: material_count
        - æ¶ˆè€—è´¡çŒ®: total_cost
        - D0 Roas: d0_roas
        - çˆ†æ¬¾ç‡: hot_rate (>$500 ä¸” ROI è¾¾æ ‡)
        - Top ç´ æ: top_material

        Returns:
            {
                'start_date': å¼€å§‹æ—¥æœŸ,
                'end_date': ç»“æŸæ—¥æœŸ,
                'summary': {total_editors, total_materials, total_cost, avg_roas},
                'editors': [å‰ªè¾‘å¸ˆæ•°æ®åˆ—è¡¨]
            }
        """
        print(f"[XMP] ç”Ÿæˆå‰ªè¾‘å¸ˆäº§å‡ºæŠ¥è¡¨: {start_date} ~ {end_date}")

        # è·å–å‰ªè¾‘å¸ˆç»Ÿè®¡æ•°æ®
        editors = self.fetch_editor_stats(
            start_date=start_date,
            end_date=end_date,
            hot_threshold=hot_threshold,
            roas_threshold=roas_threshold
        )

        if not editors:
            return {
                'start_date': start_date,
                'end_date': end_date,
                'summary': {},
                'editors': []
            }

        # è®¡ç®—æ±‡æ€»æ•°æ®
        total_cost = sum(e['total_cost'] for e in editors)
        total_revenue = sum(e['total_revenue'] for e in editors)
        total_materials = sum(e['material_count'] for e in editors)

        summary = {
            'total_editors': len(editors),
            'total_materials': total_materials,
            'total_cost': total_cost,
            'total_revenue': total_revenue,
            'avg_roas': total_revenue / total_cost if total_cost > 0 else 0
        }

        # æ ¼å¼åŒ–è¾“å‡º
        print(f"\n{'='*60}")
        print(f"å‰ªè¾‘å¸ˆäº§å‡ºä¸è´¨é‡ (Editor Performance)")
        print(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        print(f"{'='*60}")
        print(f"{'å‰ªè¾‘å¸ˆ':<12} {'ä¸Šæ–°ç´ ææ•°':>10} {'æ¶ˆè€—è´¡çŒ®':>12} {'D0 ROAS':>10} {'çˆ†æ¬¾ç‡':>10} {'Top ç´ æ':<20}")
        print(f"{'-'*60}")

        for e in editors[:10]:  # åªæ‰“å°å‰10
            name = e['name'][:10] if len(e['name']) > 10 else e['name']
            top_mat = e['top_material'][:18] if len(e['top_material']) > 18 else e['top_material']
            print(f"{name:<12} {e['material_count']:>10} ${e['total_cost']:>10,.0f} {e['d0_roas']*100:>9.1f}% {e['hot_rate']*100:>9.1f}% {top_mat:<20}")

        print(f"{'='*60}")
        print(f"æ±‡æ€»: {summary['total_editors']} ä½å‰ªè¾‘å¸ˆ, {total_materials} ä¸ªç´ æ, æ€»æ¶ˆè€— ${total_cost:,.0f}, å¹³å‡ ROAS {summary['avg_roas']*100:.1f}%")

        return {
            'start_date': start_date,
            'end_date': end_date,
            'summary': summary,
            'editors': editors
        }

    def fetch_daily_editor_output(self, date: str) -> Dict[str, Any]:
        """
        è·å–æŸå¤©çš„å‰ªè¾‘å¸ˆäº§å‡ºç»Ÿè®¡ (æ—¥æŠ¥ç”¨)

        Args:
            date: æ—¥æœŸ YYYY-MM-DD

        Returns:
            {
                'date': æ—¥æœŸ,
                'total_materials': æ€»ç´ ææ•°,
                'editors': [{name, count, materials}]
            }
        """
        print(f"[XMP] è·å–å‰ªè¾‘å¸ˆæ—¥äº§å‡º: {date}")

        # è·å–å½“å¤©åˆ›å»ºçš„ç´ æ
        materials = self.fetch_material_details(
            start_date=date,
            end_date=date,
            is_deleted=0
        )

        if not materials:
            return {'date': date, 'total_materials': 0, 'editors': []}

        # æŒ‰å‰ªè¾‘å¸ˆèšåˆ
        editor_output = {}
        for mat in materials:
            designer_raw = mat.get("designer", [])
            if isinstance(designer_raw, list) and designer_raw:
                designer = designer_raw[0].get("name", "æœªçŸ¥")
            elif isinstance(designer_raw, dict):
                designer = designer_raw.get("name", "æœªçŸ¥")
            else:
                designer = "æœªçŸ¥"

            if designer not in editor_output:
                editor_output[designer] = {
                    "name": designer,
                    "count": 0,
                    "materials": []
                }

            editor_output[designer]["count"] += 1
            editor_output[designer]["materials"].append({
                "name": mat.get("material_name"),
                "shape": mat.get("shape"),
                "duration": mat.get("duration"),
                "folder": mat.get("folder_name")
            })

        # è½¬ä¸ºåˆ—è¡¨å¹¶æ’åº
        editors = list(editor_output.values())
        editors.sort(key=lambda x: x["count"], reverse=True)

        result = {
            'date': date,
            'total_materials': len(materials),
            'editors': editors
        }

        print(f"[XMP] æ—¥äº§å‡ºç»Ÿè®¡: {len(materials)} ä¸ªç´ æ, {len(editors)} ä½å‰ªè¾‘å¸ˆ")
        return result


async def run_once(date_str: str = None, upload_bq: bool = False):
    """æ‰§è¡Œä¸€æ¬¡æŠ“å–ï¼ˆåŒ…å«æŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡ï¼‰"""
    if not date_str:
        date_str = datetime.now().strftime('%Y-%m-%d')

    scraper = XMPMultiChannelScraper()
    result = await scraper.fetch_all_channels(date_str, date_str)

    campaigns = result.get('campaigns', [])
    if not campaigns:
        print("[XMP] æ—  campaign æ•°æ®")
        return result

    # èšåˆæŠ•æ‰‹ç»Ÿè®¡ (ä½¿ç”¨ summary API)
    optimizer_stats = await fetch_optimizer_summary_stats(scraper.bearer_token, date_str)

    # è·å–å‰ªè¾‘å¸ˆæ•°æ® (åˆ†æ¸ é“ï¼Œä¸åŒæ¸ é“ç”¨ä¸åŒæ–¹å¼)
    editor_stats = []

    # Meta: ä½¿ç”¨ designer ç»´åº¦ API (åŒæ—¶è·å– is_xmp=0 å’Œ is_xmp=1 çš„æ•°æ®)
    meta_designers = []

    # è·å– is_xmp=0 çš„å‰ªè¾‘å¸ˆ
    designers_0 = await scraper.fetch_channel_designers('facebook', date_str, date_str, is_xmp="0")
    if designers_0:
        meta_designers.extend(designers_0)

    # è·å– is_xmp=1 çš„å‰ªè¾‘å¸ˆ
    designers_1 = await scraper.fetch_channel_designers('facebook', date_str, date_str, is_xmp="1")
    if designers_1:
        meta_designers.extend(designers_1)

    if meta_designers:
        for d in meta_designers:
            cost = d['cost']
            revenue = d['revenue']
            editor_stats.append({
                'stat_date': date_str,
                'channel': 'facebook',
                'name': d['designer_name'],
                'total_cost': cost,
                'total_revenue': revenue,
                'd0_roas': revenue / cost if cost > 0 else 0,
                'impressions': d['impression'],
                'clicks': d['click'],
                'material_count': 0,
                'hot_count': 0,
                'hot_rate': 0,
                'top_material': '',
                'top_material_cost': 0,
                'top_material_roas': 0,
            })

    # TikTok: ä½¿ç”¨ ad ç»´åº¦ APIï¼Œä» ad_name è§£æå‰ªè¾‘å¸ˆ
    tk_ads = await scraper.fetch_channel_ads('tiktok', date_str, date_str)
    if tk_ads:
        tk_editor_stats = aggregate_editor_stats_from_ads(tk_ads, date_str)
        editor_stats.extend(tk_editor_stats)

    # æŒ‰æ¶ˆè€—æ’åº
    editor_stats.sort(key=lambda x: x['total_cost'], reverse=True)

    # è®¡ç®—å»é‡åçš„äººæ•°
    unique_optimizers = len(set(o['name'] for o in optimizer_stats))
    print(f"[XMP] æŠ•æ‰‹ç»Ÿè®¡: {unique_optimizers} äºº ({len(optimizer_stats)} æ¡è®°å½•), å‰ªè¾‘å¸ˆç»Ÿè®¡: {len(editor_stats)} äºº")

    result['optimizer_stats'] = optimizer_stats
    result['editor_stats'] = editor_stats

    # ä¸Šä¼ åˆ° BigQuery
    if upload_bq:
        try:
            from bigquery_storage import BigQueryUploader

            project_id = os.getenv('BQ_PROJECT_ID')
            dataset_id = os.getenv('BQ_DATASET_ID', 'xmp_data')

            if project_id:
                batch_id = datetime.now(BEIJING_TZ).strftime('%Y%m%d_%H%M%S')
                uploader = BigQueryUploader(project_id, dataset_id)

                # ä¸Šä¼  campaign æ•°æ®
                count1 = uploader.upload_xmp_internal_campaigns(campaigns, batch_id=batch_id)
                print(f"[BQ] å·²ä¸Šä¼  {count1} æ¡ campaign è®°å½•")

                # ä¸Šä¼ æŠ•æ‰‹ç»Ÿè®¡
                count2 = uploader.upload_optimizer_stats(optimizer_stats, batch_id=batch_id)
                print(f"[BQ] å·²ä¸Šä¼  {count2} æ¡æŠ•æ‰‹ç»Ÿè®¡")

                # ä¸Šä¼ å‰ªè¾‘å¸ˆç»Ÿè®¡
                count3 = uploader.upload_editor_stats(editor_stats, batch_id=batch_id)
                print(f"[BQ] å·²ä¸Šä¼  {count3} æ¡å‰ªè¾‘å¸ˆç»Ÿè®¡")
            else:
                print("[BQ] æœªé…ç½® BQ_PROJECT_IDï¼Œè·³è¿‡ä¸Šä¼ ")
        except Exception as e:
            print(f"[BQ] ä¸Šä¼ å¤±è´¥: {e}")
            send_lark_alert("XMP æ•°æ®ä¸Šä¼ å¤±è´¥", str(e), level="error")

    return result


async def fetch_optimizer_summary_stats(
    bearer_token: str,
    date_str: str,
    optimizer_list: List[str] = None
) -> List[Dict]:
    """
    ä½¿ç”¨ channel/summary API è·å–æŠ•æ‰‹æ±‡æ€»æ•°æ®

    Args:
        bearer_token: Bearer Token
        date_str: æ—¥æœŸ YYYY-MM-DD
        optimizer_list: æŠ•æ‰‹åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨ OPTIMIZER_LIST

    Returns:
        æŠ•æ‰‹ç»Ÿè®¡åˆ—è¡¨
    """
    if optimizer_list is None:
        optimizer_list = OPTIMIZER_LIST

    channels = ['tiktok', 'facebook']
    results = []

    headers = {
        "Authorization": bearer_token,  # bearer_token å·²åŒ…å« "Bearer " å‰ç¼€
        "Content-Type": "application/json",
        "Origin": "https://xmp.mobvista.com",
        "Referer": "https://xmp.mobvista.com/"
    }

    for optimizer in optimizer_list:
        optimizer_data = {
            'stat_date': date_str,
            'name': optimizer,
            'tiktok_cost': 0,
            'tiktok_revenue': 0,
            'tiktok_roas': 0,
            'facebook_cost': 0,
            'facebook_revenue': 0,
            'facebook_roas': 0,
            'total_cost': 0,
            'total_revenue': 0,
            'roas': 0
        }

        for channel in channels:
            payload = {
                "level": "campaign",
                "channel": channel,
                "start_time": date_str,
                "end_time": date_str,
                "field": "cost,purchase_value,total_complete_payment_rate",
                "page": 1,
                "page_size": 1000,
                "report_timezone": "",
                "search": [
                    {"item": "campaign", "val": optimizer, "op": "LIKE", "op_type": "OR"}
                ]
            }

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        XMP_SUMMARY_URL,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        result = await response.json()

                        if result.get('code') != 0:
                            print(f"[XMP] API é”™è¯¯ ({optimizer}/{channel}): {result.get('msg')}")
                            continue

                        sum_data = result.get('data', {}).get('sum', {})
                        cost = float(sum_data.get('cost', 0) or 0)
                        # TikTok ç”¨ total_complete_payment_rateï¼ŒFacebook ç”¨ purchase_value
                        if channel == 'tiktok':
                            revenue = float(sum_data.get('total_complete_payment_rate', 0) or 0)
                        else:
                            revenue = float(sum_data.get('purchase_value', 0) or 0)
                        roas = revenue / cost if cost > 0 else 0

                        if channel == 'tiktok':
                            optimizer_data['tiktok_cost'] = cost
                            optimizer_data['tiktok_revenue'] = revenue
                            optimizer_data['tiktok_roas'] = roas
                        else:
                            optimizer_data['facebook_cost'] = cost
                            optimizer_data['facebook_revenue'] = revenue
                            optimizer_data['facebook_roas'] = roas

            except Exception as e:
                print(f"[XMP] è¯·æ±‚å¤±è´¥ ({optimizer}/{channel}): {e}")

        # åˆ†æ¸ é“æ·»åŠ è®°å½• (export_stats_to_lark_doc éœ€è¦æŒ‰ channel åŒºåˆ†)
        # TikTok è®°å½•
        if optimizer_data['tiktok_cost'] > 0 or optimizer_data['tiktok_revenue'] > 0:
            results.append({
                'stat_date': date_str,
                'name': optimizer,
                'channel': 'tiktok',
                'total_cost': optimizer_data['tiktok_cost'],
                'total_revenue': optimizer_data['tiktok_revenue'],
                'roas': optimizer_data['tiktok_roas'],
            })

        # Facebook è®°å½•
        if optimizer_data['facebook_cost'] > 0 or optimizer_data['facebook_revenue'] > 0:
            results.append({
                'stat_date': date_str,
                'name': optimizer,
                'channel': 'facebook',
                'total_cost': optimizer_data['facebook_cost'],
                'total_revenue': optimizer_data['facebook_revenue'],
                'roas': optimizer_data['facebook_roas'],
            })

        print(f"[XMP] {optimizer}: TT ${optimizer_data['tiktok_cost']:,.2f}, Meta ${optimizer_data['facebook_cost']:,.2f}")

    return results


async def fetch_editor_tiktok_stats(
    bearer_token: str,
    date_str: str
) -> List[Dict]:
    """
    ä½¿ç”¨ channel/summary API è·å–å‰ªè¾‘å¸ˆ TikTok æ•°æ®

    Args:
        bearer_token: Bearer Token
        date_str: æ—¥æœŸ YYYY-MM-DD

    Returns:
        å‰ªè¾‘å¸ˆ TikTok ç»Ÿè®¡åˆ—è¡¨
    """
    results = []

    headers = {
        "Authorization": bearer_token,  # bearer_token å·²åŒ…å« "Bearer " å‰ç¼€
        "Content-Type": "application/json",
        "Origin": "https://xmp.mobvista.com",
        "Referer": "https://xmp.mobvista.com/"
    }

    for cn_name, aliases in EDITOR_NAME_MAP.items():
        # æœç´¢æ‰€æœ‰å¯èƒ½çš„åå­—ï¼ˆè‹±æ–‡å + ä¸­æ–‡å + å§“æ°ï¼‰
        search_names = [cn_name] + aliases  # ä¸­æ–‡å…¨å + æ‰€æœ‰åˆ«å

        total_cost = 0
        total_revenue = 0

        for search_name in search_names:
            payload = {
                "level": "ad",
                "channel": "tiktok",
                "start_time": date_str,
                "end_time": date_str,
                "field": "cost,total_complete_payment_rate",
                "page": 1,
                "page_size": 1000,
                "report_timezone": "",
                "search": [
                    {"item": "ad", "val": search_name, "op": "LIKE", "op_type": "OR"}
                ]
            }

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        XMP_SUMMARY_URL,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        result = await response.json()

                        if result.get('code') != 0:
                            continue

                        sum_data = result.get('data', {}).get('sum', {})
                        cost = float(sum_data.get('cost', 0) or 0)
                        revenue = float(sum_data.get('total_complete_payment_rate', 0) or 0)

                        total_cost += cost
                        total_revenue += revenue

            except Exception as e:
                print(f"[XMP] è¯·æ±‚å¤±è´¥ ({cn_name}/{search_name}): {e}")

        # æ±‡æ€»è¯¥å‰ªè¾‘å¸ˆæ‰€æœ‰åå­—çš„æ•°æ®
        roas = total_revenue / total_cost if total_cost > 0 else 0
        results.append({
            'editor_name': cn_name,
            'channel': 'tiktok',
            'stat_date': date_str,
            'spend': total_cost,
            'revenue': total_revenue,
            'roas': roas
        })
        print(f"[XMP] {cn_name} (TT): ${total_cost:,.2f}, Rev ${total_revenue:,.2f}")

    return results


async def fetch_editor_facebook_stats(
    bearer_token: str,
    date_str: str
) -> List[Dict]:
    """
    ä½¿ç”¨ channel/list API è·å–å‰ªè¾‘å¸ˆ Facebook æ•°æ®

    Args:
        bearer_token: Bearer Token
        date_str: æ—¥æœŸ YYYY-MM-DD

    Returns:
        å‰ªè¾‘å¸ˆ Facebook ç»Ÿè®¡åˆ—è¡¨
    """
    results = []

    headers = {
        "Authorization": bearer_token,  # bearer_token å·²åŒ…å« "Bearer " å‰ç¼€
        "Content-Type": "application/json",
        "Origin": "https://xmp.mobvista.com",
        "Referer": "https://xmp.mobvista.com/"
    }

    # ä½¿ç”¨ designer level è·å–æ‰€æœ‰å‰ªè¾‘å¸ˆæ•°æ®
    payload = {
        "level": "designer",
        "channel": "facebook",
        "start_time": date_str,
        "end_time": date_str,
        "field": "cost,purchase_value,designer_name",
        "page": 1,
        "page_size": 1000,
        "report_timezone": "",
        "search": [
            {"item": "is_xmp", "val": "0", "op": "EQ"}
        ],
        "source": "report"
    }

    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                XMP_LIST_URL,
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                result = await response.json()

                if result.get('code') != 0:
                    print(f"[XMP] API é”™è¯¯: {result.get('msg')}")
                    return results

                data_list = result.get('data', {}).get('list', [])

                # æ„å»ºå‰ªè¾‘å¸ˆä¸­æ–‡ååˆ°è‹±æ–‡åçš„åå‘æ˜ å°„
                name_to_cn = {}
                for cn_name, aliases in EDITOR_NAME_MAP.items():
                    name_to_cn[cn_name] = cn_name
                    for alias in aliases:
                        name_to_cn[alias.lower()] = cn_name

                # å®‰å…¨è½¬æ¢ä¸º float
                def safe_float(val):
                    if val is None or val == '' or val == '-':
                        return 0.0
                    try:
                        return float(val)
                    except (ValueError, TypeError):
                        return 0.0

                # æŒ‰å‰ªè¾‘å¸ˆèšåˆæ•°æ®
                editor_data = {}
                for item in data_list:
                    designer_name = item.get('designer_name', '')
                    # ä½¿ç”¨ currency_cost è€Œä¸æ˜¯ costï¼ˆcost è¿”å› '-'ï¼‰
                    cost = safe_float(item.get('currency_cost', 0))
                    revenue = safe_float(item.get('purchase_value', 0))

                    # æŸ¥æ‰¾å¯¹åº”çš„ä¸­æ–‡å
                    cn_name = name_to_cn.get(designer_name.lower())
                    if not cn_name:
                        cn_name = name_to_cn.get(designer_name)

                    if cn_name:
                        if cn_name not in editor_data:
                            editor_data[cn_name] = {'cost': 0, 'revenue': 0}
                        editor_data[cn_name]['cost'] += cost
                        editor_data[cn_name]['revenue'] += revenue

                # è½¬æ¢ä¸ºç»“æœåˆ—è¡¨
                for cn_name in EDITOR_NAME_MAP.keys():
                    data = editor_data.get(cn_name, {'cost': 0, 'revenue': 0})
                    cost = data['cost']
                    revenue = data['revenue']
                    roas = revenue / cost if cost > 0 else 0

                    results.append({
                        'editor_name': cn_name,
                        'channel': 'facebook',
                        'stat_date': date_str,
                        'spend': cost,
                        'revenue': revenue,
                        'roas': roas
                    })
                    print(f"[XMP] {cn_name} (Meta): ${cost:,.2f}, Rev ${revenue:,.2f}")

    except Exception as e:
        print(f"[XMP] è¯·æ±‚å¤±è´¥ (Facebook designer): {e}")

    return results


async def fetch_editor_combined_stats(
    bearer_token: str,
    date_str: str
) -> List[Dict]:
    """
    è·å–å‰ªè¾‘å¸ˆå®Œæ•´æ•°æ®ï¼ˆTikTok + Facebook åˆå¹¶ï¼‰

    Args:
        bearer_token: Bearer Token
        date_str: æ—¥æœŸ YYYY-MM-DD

    Returns:
        å‰ªè¾‘å¸ˆå®Œæ•´ç»Ÿè®¡åˆ—è¡¨
    """
    # å¹¶è¡Œè·å– TikTok å’Œ Facebook æ•°æ®
    tt_data = await fetch_editor_tiktok_stats(bearer_token, date_str)
    fb_data = await fetch_editor_facebook_stats(bearer_token, date_str)

    # åˆå¹¶æ•°æ®
    results = []
    tt_map = {d['editor_name']: d for d in tt_data}
    fb_map = {d['editor_name']: d for d in fb_data}

    for cn_name in EDITOR_NAME_MAP.keys():
        tt = tt_map.get(cn_name, {'spend': 0, 'revenue': 0})
        fb = fb_map.get(cn_name, {'spend': 0, 'revenue': 0})

        tt_spend = tt.get('spend', 0)
        tt_revenue = tt.get('revenue', 0)
        fb_spend = fb.get('spend', 0)
        fb_revenue = fb.get('revenue', 0)

        total_spend = tt_spend + fb_spend
        total_revenue = tt_revenue + fb_revenue

        results.append({
            'editor_name': cn_name,
            'stat_date': date_str,
            'tt_spend': tt_spend,
            'tt_revenue': tt_revenue,
            'tt_roas': tt_revenue / tt_spend if tt_spend > 0 else 0,
            'meta_spend': fb_spend,
            'meta_revenue': fb_revenue,
            'meta_roas': fb_revenue / fb_spend if fb_spend > 0 else 0,
            'total_spend': total_spend,
            'total_revenue': total_revenue,
            'total_roas': total_revenue / total_spend if total_spend > 0 else 0
        })

    return results


def aggregate_optimizer_stats(campaigns: List[Dict], date_str: str) -> List[Dict]:
    """
    ä» campaign æ•°æ®èšåˆæŠ•æ‰‹ç»Ÿè®¡

    Args:
        campaigns: campaign åˆ—è¡¨
        date_str: ç»Ÿè®¡æ—¥æœŸ

    Returns:
        æŠ•æ‰‹ç»Ÿè®¡åˆ—è¡¨
    """
    optimizer_data = {}

    for c in campaigns:
        channel = c.get('channel', '')
        campaign_name = c.get('campaign_name', '')

        # ä» campaign_name æå–æŠ•æ‰‹å (æ ¼å¼: optimizer-xxx)
        optimizer = None
        match = re.search(r'optimizer-([a-zA-Z]+)', campaign_name, re.IGNORECASE)
        if match:
            optimizer = match.group(1).lower()

        if not optimizer:
            continue

        key = (channel, optimizer)
        cost = float(c.get('cost', 0))
        revenue = float(c.get('revenue', 0))

        if key not in optimizer_data:
            optimizer_data[key] = {
                'channel': channel,
                'name': optimizer,
                'stat_date': date_str,
                'campaign_count': 0,
                'total_cost': 0,
                'total_revenue': 0,
                'impressions': 0,
                'clicks': 0,
                'conversions': 0,
                'campaigns': []
            }

        optimizer_data[key]['campaign_count'] += 1
        optimizer_data[key]['total_cost'] += cost
        optimizer_data[key]['total_revenue'] += revenue
        optimizer_data[key]['impressions'] += float(c.get('impression', 0))
        optimizer_data[key]['clicks'] += float(c.get('click', 0))
        optimizer_data[key]['campaigns'].append({
            'name': campaign_name,
            'cost': cost,
            'revenue': revenue,
            'roas': revenue / cost if cost > 0 else 0
        })

    # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
    result = []
    for key, data in optimizer_data.items():
        cost = data['total_cost']
        revenue = data['total_revenue']

        # æ‰¾ Top campaign
        campaigns_sorted = sorted(data['campaigns'], key=lambda x: x['cost'], reverse=True)
        top = campaigns_sorted[0] if campaigns_sorted else None

        result.append({
            'stat_date': date_str,
            'channel': data['channel'],
            'name': data['name'],
            'campaign_count': data['campaign_count'],
            'total_cost': cost,
            'total_revenue': revenue,
            'roas': revenue / cost if cost > 0 else 0,
            'impressions': data['impressions'],
            'clicks': data['clicks'],
            'conversions': data['conversions'],
            'top_campaign': top['name'] if top else '',
            'top_campaign_spend': top['cost'] if top else 0,
            'top_campaign_roas': top['roas'] if top else 0,
        })

    result.sort(key=lambda x: x['total_cost'], reverse=True)
    return result


def aggregate_editor_stats_from_campaigns(campaigns: List[Dict], date_str: str) -> List[Dict]:
    """
    ä» campaign æ•°æ®èšåˆå‰ªè¾‘å¸ˆç»Ÿè®¡ (TikTok ä»å¹¿å‘Šåç§°è§£æ)

    Args:
        campaigns: campaign åˆ—è¡¨
        date_str: ç»Ÿè®¡æ—¥æœŸ

    Returns:
        å‰ªè¾‘å¸ˆç»Ÿè®¡åˆ—è¡¨
    """
    editor_data = {}

    for c in campaigns:
        channel = c.get('channel', '')
        campaign_name = c.get('campaign_name', '')

        # ä» campaign_name æå–å‰ªè¾‘å¸ˆå
        editor = None
        # å…ˆåŒ¹é…ä¸­æ–‡å
        for cn_name in EDITOR_NAME_MAP.keys():
            if cn_name in campaign_name:
                editor = cn_name
                break
        # å†åŒ¹é…åˆ«å
        if not editor:
            name_lower = campaign_name.lower()
            for alias, cn_name in EDITOR_ALIAS_MAP.items():
                if alias.lower() in name_lower:
                    editor = cn_name
                    break

        if not editor:
            continue

        key = (channel, editor)
        cost = float(c.get('cost', 0))
        revenue = float(c.get('revenue', 0))
        roas = revenue / cost if cost > 0 else 0

        if key not in editor_data:
            editor_data[key] = {
                'channel': channel,
                'name': editor,
                'stat_date': date_str,
                'material_count': 0,
                'total_cost': 0,
                'total_revenue': 0,
                'impressions': 0,
                'clicks': 0,
                'hot_count': 0,
                'materials': []
            }

        editor_data[key]['material_count'] += 1
        editor_data[key]['total_cost'] += cost
        editor_data[key]['total_revenue'] += revenue
        editor_data[key]['impressions'] += float(c.get('impression', 0))
        editor_data[key]['clicks'] += float(c.get('click', 0))

        # çˆ†æ¬¾åˆ¤å®š: æ¶ˆè€— > $500 ä¸” ROAS >= 45%
        if cost >= 500 and roas >= 0.45:
            editor_data[key]['hot_count'] += 1

        editor_data[key]['materials'].append({
            'name': campaign_name,
            'cost': cost,
            'revenue': revenue,
            'roas': roas
        })

    # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
    result = []
    for key, data in editor_data.items():
        cost = data['total_cost']
        revenue = data['total_revenue']
        count = data['material_count']

        # æ‰¾ Top ç´ æ
        materials_sorted = sorted(data['materials'], key=lambda x: x['cost'], reverse=True)
        top = materials_sorted[0] if materials_sorted else None

        result.append({
            'stat_date': date_str,
            'channel': data['channel'],
            'name': data['name'],
            'material_count': count,
            'total_cost': cost,
            'total_revenue': revenue,
            'd0_roas': revenue / cost if cost > 0 else 0,
            'impressions': data['impressions'],
            'clicks': data['clicks'],
            'hot_count': data['hot_count'],
            'hot_rate': data['hot_count'] / count if count > 0 else 0,
            'top_material': top['name'] if top else '',
            'top_material_cost': top['cost'] if top else 0,
            'top_material_roas': top['roas'] if top else 0,
        })

    result.sort(key=lambda x: x['total_cost'], reverse=True)
    return result


def aggregate_editor_stats_from_ads(ads: List[Dict], date_str: str) -> List[Dict]:
    """
    ä»å¹¿å‘Šç»´åº¦æ•°æ®èšåˆå‰ªè¾‘å¸ˆç»Ÿè®¡ (åˆ†æ¸ é“)

    å¹¿å‘Šåç§°æ ¼å¼:
    - 12.25-å®‹-Eldest Daughter's Marriage Life-ko-4.mp4
    - 15000696_ja_vc_beita_1229_hilight_3.mp4

    Args:
        ads: å¹¿å‘Šåˆ—è¡¨ (åŒ…å« channel, ad_name, cost ç­‰)
        date_str: ç»Ÿè®¡æ—¥æœŸ

    Returns:
        å‰ªè¾‘å¸ˆç»Ÿè®¡åˆ—è¡¨ (åˆ†æ¸ é“)
    """
    editor_data = {}

    for ad in ads:
        channel = ad.get('channel', '')
        ad_name = ad.get('ad_name', '')

        # ä» ad_name æå–å‰ªè¾‘å¸ˆå
        editor = extract_editor_from_ad_name(ad_name)

        if not editor:
            continue

        key = (channel, editor)
        cost = float(ad.get('cost', 0))
        revenue = float(ad.get('revenue', 0))

        if key not in editor_data:
            editor_data[key] = {
                'channel': channel,
                'name': editor,
                'stat_date': date_str,
                'material_count': 0,
                'total_cost': 0,
                'total_revenue': 0,
                'impressions': 0,
                'clicks': 0,
                'conversions': 0,
                'ads': []
            }

        editor_data[key]['material_count'] += 1
        editor_data[key]['total_cost'] += cost
        editor_data[key]['total_revenue'] += revenue
        editor_data[key]['impressions'] += float(ad.get('impression', 0))
        editor_data[key]['clicks'] += float(ad.get('click', 0))
        editor_data[key]['conversions'] += float(ad.get('conversion', 0))
        editor_data[key]['ads'].append({
            'name': ad_name,
            'cost': cost,
            'revenue': revenue,
        })

    # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
    result = []
    for key, data in editor_data.items():
        cost = data['total_cost']
        revenue = data['total_revenue']
        count = data['material_count']
        d0_roas = revenue / cost if cost > 0 else 0

        # æ‰¾ Top ç´ æ (æŒ‰æ¶ˆè€—æ’åº)
        ads_sorted = sorted(data['ads'], key=lambda x: x['cost'], reverse=True)
        top = ads_sorted[0] if ads_sorted else None
        top_roas = top['revenue'] / top['cost'] if top and top['cost'] > 0 else 0

        result.append({
            'stat_date': date_str,
            'channel': data['channel'],
            'name': data['name'],
            'material_count': count,
            'total_cost': cost,
            'total_revenue': revenue,
            'd0_roas': d0_roas,
            'impressions': data['impressions'],
            'clicks': data['clicks'],
            'hot_count': 0,
            'hot_rate': 0,
            'top_material': top['name'] if top else '',
            'top_material_cost': top['cost'] if top else 0,
            'top_material_roas': top_roas,
        })

    result.sort(key=lambda x: x['total_cost'], reverse=True)
    print(f"[XMP] ä»å¹¿å‘Šæ•°æ®æå–å‰ªè¾‘å¸ˆ: {len(result)} æ¡ (åˆ†æ¸ é“)")
    return result


def extract_editor_from_ad_name(ad_name: str) -> Optional[str]:
    """
    ä»å¹¿å‘Šåç§°ä¸­æå–å‰ªè¾‘å¸ˆå

    æ”¯æŒä¸¤ç§æ ¼å¼:
    1. 12.25-å®‹-Eldest Daughter's Marriage Life-ko-4.mp4
    2. 15000696_ja_vc_beita_1229_hilight_3.mp4
    """
    if not ad_name:
        return None

    # æ ¼å¼1: æ—¥æœŸ-å‰ªè¾‘å¸ˆ-å‰§å-è¯­è¨€-åºå·.mp4
    if '-' in ad_name:
        parts = ad_name.split('-')
        if len(parts) >= 2:
            editor_part = parts[1].strip()
            # åŒ¹é…ä¸­æ–‡åæˆ–åˆ«å
            if editor_part in EDITOR_ALIAS_MAP:
                return EDITOR_ALIAS_MAP[editor_part]
            # åŒ¹é…å•å§“
            for cn_name, aliases in EDITOR_NAME_MAP.items():
                if editor_part == cn_name:
                    return cn_name
                for alias in aliases:
                    if alias.lower() == editor_part.lower():
                        return cn_name

    # æ ¼å¼2: dramaid_lang_vc_editor_date_xxx.mp4
    if '_' in ad_name:
        parts = ad_name.split('_')
        for part in parts:
            part_lower = part.lower()
            # åŒ¹é…è‹±æ–‡å
            for cn_name, aliases in EDITOR_NAME_MAP.items():
                for alias in aliases:
                    if alias.lower() == part_lower:
                        return cn_name

    return None


def aggregate_editor_stats_from_material_report(date_str: str) -> List[Dict]:
    """
    ä»ç´ ææŠ¥è¡¨èšåˆå‰ªè¾‘å¸ˆç»Ÿè®¡

    ç´ æåç§°æ ¼å¼: æ—¥æœŸ-å‰ªè¾‘å¸ˆå-å‰§å-è¯­è¨€-åºå·.mp4
    """
    scraper = XMPEditorStatsScraper()
    material_data = scraper.fetch_material_report(date_str, date_str)

    if not material_data:
        print("[XMP] ç´ ææŠ¥è¡¨ä¸ºç©º")
        return []

    editor_data = {}

    for item in material_data:
        material_name = item.get('material_name', '')
        editor = extract_editor_from_material_name(material_name)

        if not editor:
            continue

        cost = float(item.get('currency_cost', 0))
        revenue = float(item.get('total_purchase_value', 0))
        roas = revenue / cost if cost > 0 else 0

        # ç´ ææŠ¥è¡¨ä¸åŒºåˆ†æ¸ é“ï¼Œç»Ÿä¸€å½’ç±»
        channel = 'all'
        key = (channel, editor)

        if key not in editor_data:
            editor_data[key] = {
                'channel': channel,
                'name': editor,
                'stat_date': date_str,
                'material_count': 0,
                'total_cost': 0,
                'total_revenue': 0,
                'hot_count': 0,
                'materials': []
            }

        editor_data[key]['material_count'] += 1
        editor_data[key]['total_cost'] += cost
        editor_data[key]['total_revenue'] += revenue

        # çˆ†æ¬¾åˆ¤å®š: æ¶ˆè€— > $500 ä¸” ROAS >= 45%
        if cost >= 500 and roas >= 0.45:
            editor_data[key]['hot_count'] += 1

        editor_data[key]['materials'].append({
            'name': material_name,
            'cost': cost,
            'revenue': revenue,
            'roas': roas
        })

    # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
    result = []
    for key, data in editor_data.items():
        cost = data['total_cost']
        revenue = data['total_revenue']
        count = data['material_count']

        # æ‰¾ Top ç´ æ
        materials_sorted = sorted(data['materials'], key=lambda x: x['cost'], reverse=True)
        top = materials_sorted[0] if materials_sorted else None

        result.append({
            'stat_date': date_str,
            'channel': data['channel'],
            'name': data['name'],
            'material_count': count,
            'total_cost': cost,
            'total_revenue': revenue,
            'd0_roas': revenue / cost if cost > 0 else 0,
            'hot_count': data['hot_count'],
            'hot_rate': data['hot_count'] / count if count > 0 else 0,
            'top_material': top['name'] if top else '',
            'top_material_cost': top['cost'] if top else 0,
            'top_material_roas': top['roas'] if top else 0,
        })

    result.sort(key=lambda x: x['total_cost'], reverse=True)
    print(f"[XMP] ä»ç´ ææŠ¥è¡¨æå–å‰ªè¾‘å¸ˆ: {len(result)} äºº")
    return result


async def run_with_stats(date_str: str = None, upload_bq: bool = False):
    """æ‰§è¡ŒæŠ“å–å¹¶ç”ŸæˆæŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡"""
    if not date_str:
        date_str = datetime.now().strftime('%Y-%m-%d')

    # 1. æŠ“å– campaign æ•°æ®
    scraper = XMPMultiChannelScraper()
    result = await scraper.fetch_all_channels(date_str, date_str)

    campaigns = result.get('campaigns', [])
    if not campaigns:
        print("[XMP] æ—  campaign æ•°æ®")
        return result

    # 2. èšåˆæŠ•æ‰‹ç»Ÿè®¡ (ä½¿ç”¨ summary API)
    optimizer_stats = await fetch_optimizer_summary_stats(scraper.bearer_token, date_str)
    print(f"[XMP] æŠ•æ‰‹ç»Ÿè®¡: {len(optimizer_stats)} äºº")

    # 3. è·å–å‰ªè¾‘å¸ˆæ•°æ® (åˆ†æ¸ é“ï¼Œä¸åŒæ¸ é“ç”¨ä¸åŒæ–¹å¼)
    editor_stats = []

    # Meta: ä½¿ç”¨ designer ç»´åº¦ API (åŒæ—¶è·å– is_xmp=0 å’Œ is_xmp=1 çš„æ•°æ®)
    meta_designers = []
    designers_0 = await scraper.fetch_channel_designers('facebook', date_str, date_str, is_xmp="0")
    if designers_0:
        meta_designers.extend(designers_0)
    designers_1 = await scraper.fetch_channel_designers('facebook', date_str, date_str, is_xmp="1")
    if designers_1:
        meta_designers.extend(designers_1)

    if meta_designers:
        for d in meta_designers:
            cost = d['cost']
            revenue = d['revenue']
            editor_stats.append({
                'stat_date': date_str,
                'channel': 'facebook',
                'name': d['designer_name'],
                'total_cost': cost,
                'total_revenue': revenue,
                'd0_roas': revenue / cost if cost > 0 else 0,
                'impressions': d['impression'],
                'clicks': d['click'],
                'material_count': 0,
                'hot_count': 0,
                'hot_rate': 0,
                'top_material': '',
                'top_material_cost': 0,
                'top_material_roas': 0,
            })

    # TikTok: ä½¿ç”¨ ad ç»´åº¦ APIï¼Œä» ad_name è§£æå‰ªè¾‘å¸ˆ
    tk_ads = await scraper.fetch_channel_ads('tiktok', date_str, date_str)
    if tk_ads:
        tk_editor_stats = aggregate_editor_stats_from_ads(tk_ads, date_str)
        editor_stats.extend(tk_editor_stats)

    # æŒ‰æ¶ˆè€—æ’åº
    editor_stats.sort(key=lambda x: x['total_cost'], reverse=True)
    print(f"[XMP] å‰ªè¾‘å¸ˆç»Ÿè®¡: {len(editor_stats)} äºº")

    result['optimizer_stats'] = optimizer_stats
    result['editor_stats'] = editor_stats

    # 4. ä¸Šä¼ åˆ° BigQuery
    if upload_bq:
        try:
            from bigquery_storage import BigQueryUploader

            project_id = os.getenv('BQ_PROJECT_ID')
            dataset_id = os.getenv('BQ_DATASET_ID', 'xmp_data')

            if project_id:
                batch_id = datetime.now(BEIJING_TZ).strftime('%Y%m%d_%H%M%S')
                uploader = BigQueryUploader(project_id, dataset_id)

                # ä¸Šä¼  campaign æ•°æ®
                count1 = uploader.upload_xmp_internal_campaigns(campaigns, batch_id=batch_id)
                print(f"[BQ] å·²ä¸Šä¼  {count1} æ¡ campaign è®°å½•")

                # ä¸Šä¼ æŠ•æ‰‹ç»Ÿè®¡
                count2 = uploader.upload_optimizer_stats(optimizer_stats, batch_id=batch_id)
                print(f"[BQ] å·²ä¸Šä¼  {count2} æ¡æŠ•æ‰‹ç»Ÿè®¡")

                # ä¸Šä¼ å‰ªè¾‘å¸ˆç»Ÿè®¡
                count3 = uploader.upload_editor_stats(editor_stats, batch_id=batch_id)
                print(f"[BQ] å·²ä¸Šä¼  {count3} æ¡å‰ªè¾‘å¸ˆç»Ÿè®¡")
            else:
                print("[BQ] æœªé…ç½® BQ_PROJECT_IDï¼Œè·³è¿‡ä¸Šä¼ ")
        except Exception as e:
            print(f"[BQ] ä¸Šä¼ å¤±è´¥: {e}")
            send_lark_alert("XMP ç»Ÿè®¡æ•°æ®ä¸Šä¼ å¤±è´¥", str(e), level="error")

    return result


def query_stats_from_bq(date_str: str) -> Dict[str, List[Dict]]:
    """
    ä» BigQuery æŸ¥è¯¢æŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡æ•°æ®

    Args:
        date_str: æ—¥æœŸ YYYY-MM-DD

    Returns:
        {'optimizer_stats': [...], 'editor_stats': [...], 'batch_id': str, 'batch_valid': bool}
    """
    from google.cloud import bigquery
    from datetime import datetime, timedelta

    project_id = os.getenv('BQ_PROJECT_ID')
    if not project_id:
        print("[BQ] æœªé…ç½® BQ_PROJECT_ID")
        return {'optimizer_stats': [], 'editor_stats': [], 'batch_id': None, 'batch_valid': False}

    client = bigquery.Client(project=project_id)

    # ç›´æ¥ä½¿ç”¨æœ€æ–°çš„ batch_idï¼ˆåŒ…å«æœ€å®Œæ•´çš„æ•°æ®ï¼‰
    batch_id = None
    latest_batch_query = f"""
    SELECT MAX(batch_id) as latest_batch_id
    FROM `{project_id}.xmp_data.xmp_optimizer_stats`
    WHERE stat_date = '{date_str}'
    """
    try:
        for row in client.query(latest_batch_query):
            batch_id = row.latest_batch_id
        if batch_id:
            print(f"[BQ] âœ“ æ‰¾åˆ°æœ€æ–°æ•°æ® batch: {batch_id}")
    except Exception as e:
        print(f"[BQ] æŸ¥è¯¢ batch_id å¤±è´¥: {e}")
        return {'optimizer_stats': [], 'editor_stats': [], 'batch_id': None, 'batch_valid': False}

    if not batch_id:
        print(f"[BQ] æœªæ‰¾åˆ° {date_str} çš„æ•°æ®")
        return {'optimizer_stats': [], 'editor_stats': [], 'batch_id': None, 'batch_valid': False}

    # 2. æ ¡éªŒ batch_id æ—¶æ•ˆæ€§
    # å¯¹äº T-1 æ—¥æŠ¥ï¼Œbatch_id åº”è¯¥æ˜¯ T æ—¥ï¼ˆä»Šå¤©ï¼‰æŠ“å–çš„
    # å¦‚æœ batch_id çš„æ—¥æœŸ <= stat_dateï¼Œè¯´æ˜å‡Œæ™¨2ç‚¹çš„ä»»åŠ¡æ²¡æœ‰æ‰§è¡ŒæˆåŠŸ
    batch_valid = True
    try:
        batch_date_str = batch_id[:8]  # æå– YYYYMMDD
        batch_date = datetime.strptime(batch_date_str, '%Y%m%d').date()
        stat_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        today = datetime.now().date()

        # å¦‚æœæ˜¯æŸ¥è¯¢å†å²æ—¥æœŸï¼ˆT-1 æˆ–æ›´æ—©ï¼‰ï¼Œbatch åº”è¯¥æ˜¯ stat_date ä¹‹åæŠ“å–çš„
        if stat_date < today:
            if batch_date <= stat_date:
                print(f"[BQ] âš ï¸ æ•°æ®æ—¶æ•ˆæ€§æ ¡éªŒå¤±è´¥!")
                print(f"[BQ]   stat_date={date_str}, batchæŠ“å–æ—¥æœŸ={batch_date_str}")
                print(f"[BQ]   batch åº”è¯¥æ˜¯ {date_str} ä¹‹åæŠ“å–çš„ï¼Œä½†å®é™…æ˜¯å½“å¤©æˆ–æ›´æ—©æŠ“å–")
                print(f"[BQ]   å¯èƒ½åŸå› : å‡Œæ™¨2ç‚¹çš„ T-1 æ•°æ®é‡‡é›†ä»»åŠ¡æœªæ‰§è¡Œ")
                batch_valid = False
            else:
                print(f"[BQ] âœ“ æ•°æ®æ—¶æ•ˆæ€§æ ¡éªŒé€šè¿‡: batch åœ¨ {batch_date_str} æŠ“å–")
    except Exception as e:
        print(f"[BQ] æ ¡éªŒ batch æ—¶æ•ˆæ€§æ—¶å‡ºé”™: {e}")

    # 3. æŸ¥è¯¢æŠ•æ‰‹ç»Ÿè®¡ï¼ˆä½¿ç”¨æœ€æ–° batch_idï¼‰
    opt_query = f"""
    SELECT
        optimizer_name as name,
        channel,
        spend as total_cost,
        revenue as total_revenue,
        roas
    FROM `{project_id}.xmp_data.xmp_optimizer_stats`
    WHERE stat_date = '{date_str}' AND batch_id = '{batch_id}'
    ORDER BY spend DESC
    """

    optimizer_stats = []
    try:
        for row in client.query(opt_query):
            optimizer_stats.append({
                'name': row.name,
                'channel': row.channel,
                'total_cost': float(row.total_cost or 0),
                'total_revenue': float(row.total_revenue or 0),
                'roas': float(row.roas or 0),
            })
        print(f"[BQ] æŸ¥è¯¢åˆ° {len(optimizer_stats)} æ¡æŠ•æ‰‹æ•°æ®")
    except Exception as e:
        print(f"[BQ] æŸ¥è¯¢æŠ•æ‰‹æ•°æ®å¤±è´¥: {e}")

    # 4. æŸ¥è¯¢å‰ªè¾‘å¸ˆç»Ÿè®¡ï¼ˆä½¿ç”¨æœ€æ–° batch_idï¼‰
    editor_query = f"""
    SELECT
        editor_name as name,
        channel,
        spend as total_cost,
        revenue as total_revenue,
        roas
    FROM `{project_id}.xmp_data.xmp_editor_stats`
    WHERE stat_date = '{date_str}' AND batch_id = '{batch_id}'
    ORDER BY spend DESC
    """

    editor_stats = []
    try:
        for row in client.query(editor_query):
            editor_stats.append({
                'name': row.name,
                'channel': row.channel,
                'total_cost': float(row.total_cost or 0),
                'total_revenue': float(row.total_revenue or 0),
                'roas': float(row.roas or 0),
            })
        print(f"[BQ] æŸ¥è¯¢åˆ° {len(editor_stats)} æ¡å‰ªè¾‘å¸ˆæ•°æ®")
    except Exception as e:
        print(f"[BQ] æŸ¥è¯¢å‰ªè¾‘å¸ˆæ•°æ®å¤±è´¥: {e}")

    return {
        'optimizer_stats': optimizer_stats,
        'editor_stats': editor_stats,
        'batch_id': batch_id,
        'batch_valid': batch_valid
    }


def query_weekly_stats_from_bq(start_date: str, end_date: str) -> Dict[str, Any]:
    """
    ä» BigQuery æŸ¥è¯¢å‘¨æŠ¥æ•°æ®ï¼ˆæ±‡æ€»ä¸€å‘¨çš„æŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡ï¼‰

    Args:
        start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD

    Returns:
        {
            'optimizer_stats': [...],
            'editor_stats': [...],
            'invalid_dates': [],  # batch æ ¡éªŒå¤±è´¥çš„æ—¥æœŸ
            'all_valid': bool
        }
    """
    from datetime import datetime, timedelta

    all_opt_stats = []
    all_editor_stats = []
    invalid_dates = []

    # é€å¤©æŸ¥è¯¢æ•°æ®
    current = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')

    while current <= end:
        date_str = current.strftime('%Y-%m-%d')
        print(f"\n--- æŸ¥è¯¢ {date_str} ---")

        bq_data = query_stats_from_bq(date_str)

        if not bq_data.get('batch_valid', True):
            invalid_dates.append(date_str)
            print(f"[å‘¨æŠ¥] âš ï¸ {date_str} æ•°æ®æ ¡éªŒå¤±è´¥ï¼Œè·³è¿‡")
        else:
            all_opt_stats.extend(bq_data.get('optimizer_stats', []))
            all_editor_stats.extend(bq_data.get('editor_stats', []))

        current += timedelta(days=1)

    return {
        'optimizer_stats': all_opt_stats,
        'editor_stats': all_editor_stats,
        'invalid_dates': invalid_dates,
        'all_valid': len(invalid_dates) == 0
    }


def export_stats_to_lark_doc(
    optimizer_stats: List[Dict],
    editor_stats: List[Dict],
    date_str: str,
    doc_token: str = None
) -> bool:
    """
    å¯¼å‡ºæŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡åˆ°é£ä¹¦æ–‡æ¡£

    Args:
        optimizer_stats: æŠ•æ‰‹ç»Ÿè®¡æ•°æ®
        editor_stats: å‰ªè¾‘å¸ˆç»Ÿè®¡æ•°æ®
        date_str: æ—¥æœŸ
        doc_token: é£ä¹¦æ–‡æ¡£ tokenï¼Œä¸ä¼ åˆ™ä»ç¯å¢ƒå˜é‡ XMP_DOC_TOKEN è·å–

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        from lark.lark_doc_client import create_doc_client
    except ImportError:
        print("[é£ä¹¦] æ— æ³•å¯¼å…¥ lark_doc_client æ¨¡å—")
        return False

    doc_token = doc_token or os.getenv('XMP_DOC_TOKEN')
    if not doc_token:
        print("[é£ä¹¦] æœªé…ç½® XMP_DOC_TOKENï¼Œè·³è¿‡æ–‡æ¡£å†™å…¥")
        return False

    # åˆå¹¶ Meta + TikTok æ•°æ®
    def merge_by_name(stats: List[Dict]) -> List[Dict]:
        merged = {}
        for s in stats:
            name = s.get('name')
            if name not in merged:
                merged[name] = {
                    'name': name,
                    'meta_spend': 0, 'meta_revenue': 0,
                    'tt_spend': 0, 'tt_revenue': 0,
                    # å‰ªè¾‘å¸ˆç‰¹æœ‰å­—æ®µ
                    'material_count': 0,
                    'hot_count': 0,
                    'top_materials': [],  # å­˜å‚¨æ‰€æœ‰ç´ æï¼Œç”¨äºæ‰¾ Top
                }
            channel = s.get('channel', '')
            spend = s.get('total_cost', 0)
            revenue = s.get('total_revenue', 0)

            if channel == 'facebook':
                merged[name]['meta_spend'] += spend
                merged[name]['meta_revenue'] += revenue
            elif channel == 'tiktok':
                merged[name]['tt_spend'] += spend
                merged[name]['tt_revenue'] += revenue

            # åˆå¹¶å‰ªè¾‘å¸ˆç‰¹æœ‰å­—æ®µ
            material_count = s.get('material_count', 0)
            hot_count = s.get('hot_count', 0)
            top_material = s.get('top_material', '')
            top_material_cost = s.get('top_material_cost', 0)
            top_material_roas = s.get('top_material_roas', 0)

            merged[name]['material_count'] += material_count
            merged[name]['hot_count'] += hot_count

            # è®°å½• Top ç´ æï¼ˆç”¨äºåç»­é€‰æ‹©æ¶ˆè€—æœ€é«˜çš„ï¼‰
            if top_material:
                merged[name]['top_materials'].append({
                    'name': top_material,
                    'cost': top_material_cost,
                    'roas': top_material_roas,
                    'channel': channel
                })

        result = []
        for name, d in merged.items():
            total_spend = d['meta_spend'] + d['tt_spend']
            total_revenue = d['meta_revenue'] + d['tt_revenue']
            material_count = d['material_count']
            hot_count = d['hot_count']

            # é€‰æ‹©æ¶ˆè€—æœ€é«˜çš„ Top ç´ æ
            top_materials = d['top_materials']
            if top_materials:
                top_mat = max(top_materials, key=lambda x: x['cost'])
                top_material = top_mat['name']
                top_material_cost = top_mat['cost']
                top_material_roas = top_mat['roas']
            else:
                top_material = ''
                top_material_cost = 0
                top_material_roas = 0

            result.append({
                'name': name,
                'meta_spend': d['meta_spend'],
                'meta_revenue': d['meta_revenue'],
                'meta_roas': d['meta_revenue'] / d['meta_spend'] if d['meta_spend'] > 0 else 0,
                'tt_spend': d['tt_spend'],
                'tt_revenue': d['tt_revenue'],
                'tt_roas': d['tt_revenue'] / d['tt_spend'] if d['tt_spend'] > 0 else 0,
                'total_spend': total_spend,
                'total_revenue': total_revenue,
                'total_roas': total_revenue / total_spend if total_spend > 0 else 0,
                # å‰ªè¾‘å¸ˆç‰¹æœ‰å­—æ®µ
                'material_count': material_count,
                'hot_count': hot_count,
                'hot_rate': hot_count / material_count if material_count > 0 else 0,
                'top_material': top_material,
                'top_material_cost': top_material_cost,
                'top_material_roas': top_material_roas,
            })
        result.sort(key=lambda x: x['total_spend'], reverse=True)
        return result

    # æ ‡æ³¨ Spend/ROAS ç¬¬ä¸€
    def add_labels(data: List[Dict]) -> List[Dict]:
        if not data:
            return data
        spend_first = max(data, key=lambda x: x['total_spend'])
        qualified = [d for d in data if d['total_spend'] >= 100]
        roas_first = max(qualified, key=lambda x: x['total_roas']) if qualified else None
        for d in data:
            labels = []
            if d == spend_first:
                labels.append('Spend Top1')
            if d == roas_first:
                labels.append('ROAS Top1')
            d['label'] = ' | '.join(labels) if labels else ''
        return data

    opt_merged = merge_by_name(optimizer_stats)
    editor_merged = merge_by_name(editor_stats)

    # å…ˆè¿‡æ»¤æ‰éŸ©å›½æŠ•æ‰‹ï¼Œå†è®¡ç®—æ ‡ç­¾
    KR_OPTIMIZERS = ['juria', 'lyla', 'jade']
    opt_merged = [o for o in opt_merged if o.get('name', '').lower() not in KR_OPTIMIZERS]
    opt_merged = add_labels(opt_merged)

    # å…ˆè¿‡æ»¤æ‰éŸ©å›½å‰ªè¾‘å¸ˆï¼Œå†è®¡ç®—æ ‡ç­¾
    KR_EDITORS = ['sydney', 'dia', 'gyeommy']
    editor_merged = [e for e in editor_merged if e.get('name', '').lower() not in KR_EDITORS]
    editor_merged = add_labels(editor_merged)

    try:
        client = create_doc_client()

        # åˆ¤æ–­æ˜¯å¦æ˜¯ Wiki token (éœ€è¦å…ˆè·å–å®é™…çš„ doc_token)
        # Wiki token é€šå¸¸ä»¥ wiki/ å¼€å¤´æˆ–è€…ç›´æ¥æ˜¯ wiki_token
        wiki_token = doc_token

        # å°è¯•è·å– Wiki èŠ‚ç‚¹ä¿¡æ¯
        node_info = client.get_wiki_node_info(wiki_token)
        if node_info.get("code") == 0:
            # æ˜¯ Wiki é¡µé¢ï¼Œè·å–å®é™…çš„ doc_token
            node = node_info.get("data", {}).get("node", {})
            actual_doc_token = node.get("obj_token")
            obj_type = node.get("obj_type")
            print(f"[é£ä¹¦] Wiki èŠ‚ç‚¹: obj_token={actual_doc_token}, obj_type={obj_type}")

            if obj_type != "docx":
                print(f"[é£ä¹¦] Wiki èŠ‚ç‚¹ç±»å‹ä¸æ˜¯æ–‡æ¡£: {obj_type}")
                return False
            doc_token = actual_doc_token

        result = client.write_xmp_daily_report(
            doc_token=doc_token,
            date_str=date_str,
            optimizer_data=opt_merged,
            editor_data=editor_merged
        )
        if result.get('code') == 0:
            print(f"[é£ä¹¦] æ—¥æŠ¥å·²å†™å…¥æ–‡æ¡£: {doc_token}")
            return True
        else:
            print(f"[é£ä¹¦] å†™å…¥å¤±è´¥: {result.get('msg')}")
            return False
    except Exception as e:
        print(f"[é£ä¹¦] å†™å…¥å¼‚å¸¸: {e}")
        return False


def export_stats_to_excel(
    optimizer_stats: List[Dict],
    editor_stats: List[Dict],
    date_str: str,
    output_path: str = None
) -> str:
    """
    å¯¼å‡ºæŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡åˆ° Excel

    Excel åŒ…å«ä¸¤ä¸ª Sheet:
    - æŠ•æ‰‹æ—¥æŠ¥: Meta/TT åˆ†æ¸ é“ + æ±‡æ€» + Spend/ROAS Top1 æ ‡æ³¨
    - å‰ªè¾‘å¸ˆæ—¥æŠ¥: åŒä¸Š
    """
    try:
        import pandas as pd
    except ImportError:
        print("[Excel] éœ€è¦å®‰è£… pandas: pip install pandas openpyxl")
        return None

    filename = output_path or f"xmp_stats_{date_str.replace('-', '')}.xlsx"

    # åˆå¹¶ Meta + TikTok æ•°æ®
    def merge_by_name(stats: List[Dict]) -> List[Dict]:
        merged = {}
        for s in stats:
            name = s.get('name')
            if name not in merged:
                merged[name] = {
                    'name': name,
                    'meta_spend': 0, 'meta_revenue': 0,
                    'tt_spend': 0, 'tt_revenue': 0,
                }
            channel = s.get('channel', '')
            spend = s.get('total_cost', 0)
            revenue = s.get('total_revenue', 0)

            if channel == 'facebook':
                merged[name]['meta_spend'] += spend
                merged[name]['meta_revenue'] += revenue
            elif channel == 'tiktok':
                merged[name]['tt_spend'] += spend
                merged[name]['tt_revenue'] += revenue

        result = []
        for name, d in merged.items():
            total_spend = d['meta_spend'] + d['tt_spend']
            total_revenue = d['meta_revenue'] + d['tt_revenue']
            result.append({
                'name': name,
                'meta_spend': d['meta_spend'],
                'meta_revenue': d['meta_revenue'],
                'meta_roas': d['meta_revenue'] / d['meta_spend'] if d['meta_spend'] > 0 else 0,
                'tt_spend': d['tt_spend'],
                'tt_revenue': d['tt_revenue'],
                'tt_roas': d['tt_revenue'] / d['tt_spend'] if d['tt_spend'] > 0 else 0,
                'total_spend': total_spend,
                'total_revenue': total_revenue,
                'total_roas': total_revenue / total_spend if total_spend > 0 else 0,
            })
        result.sort(key=lambda x: x['total_spend'], reverse=True)
        return result

    # æ ‡æ³¨ Spend/ROAS ç¬¬ä¸€
    def add_labels(data: List[Dict]) -> List[Dict]:
        if not data:
            return data
        spend_first = max(data, key=lambda x: x['total_spend'])
        qualified = [d for d in data if d['total_spend'] >= 100]
        roas_first = max(qualified, key=lambda x: x['total_roas']) if qualified else None
        for d in data:
            labels = []
            if d == spend_first:
                labels.append('Spend Top1')
            if d == roas_first:
                labels.append('ROAS Top1')
            d['label'] = ' | '.join(labels) if labels else ''
        return data

    opt_merged = add_labels(merge_by_name(optimizer_stats))
    editor_merged = add_labels(merge_by_name(editor_stats))

    # å†™å…¥ Excel
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        if opt_merged:
            opt_df = pd.DataFrame(opt_merged)
            opt_df.columns = ['æŠ•æ‰‹', 'Meta Spend', 'Meta Revenue', 'Meta ROAS',
                              'TT Spend', 'TT Revenue', 'TT ROAS',
                              'æ€» Spend', 'æ€» Revenue', 'æ€» ROAS', 'æ ‡æ³¨']
            opt_df.to_excel(writer, sheet_name='æŠ•æ‰‹æ—¥æŠ¥', index=False)

        if editor_merged:
            ed_df = pd.DataFrame(editor_merged)
            ed_df.columns = ['å‰ªè¾‘å¸ˆ', 'Meta Spend', 'Meta Revenue', 'Meta ROAS',
                             'TT Spend', 'TT Revenue', 'TT ROAS',
                             'æ€» Spend', 'æ€» Revenue', 'æ€» ROAS', 'æ ‡æ³¨']
            ed_df.to_excel(writer, sheet_name='å‰ªè¾‘å¸ˆæ—¥æŠ¥', index=False)

    print(f"[Excel] å·²å¯¼å‡º: {filename}")
    return filename


def generate_weekly_summary(
    optimizer_stats: List[Dict],
    editor_stats: List[Dict],
    start_date: str,
    end_date: str,
    min_spend_threshold: float = 1000.0
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå‘¨æŠ¥æ±‡æ€»æ•°æ®

    åŠŸèƒ½:
    - ä¸€å‘¨æ•°æ®ç®€å•ç›¸åŠ æ±‡æ€»
    - ç­›é€‰æœ€ä½³æŠ•æ‰‹ 1 å (ç»¼åˆè¯„åˆ†æœ€é«˜)
    - ç­›é€‰æœ€ä½³å‰ªè¾‘å¸ˆ 1 å (ç»¼åˆè¯„åˆ†æœ€é«˜)

    è¯„åˆ†è§„åˆ™:
    - ç»¼åˆè¯„åˆ† = Spend æ’ååˆ† + ROAS æ’ååˆ†
    - åªæœ‰ Spend >= min_spend_threshold çš„æ‰å‚ä¸æœ€ä½³è¯„é€‰

    Args:
        optimizer_stats: æŠ•æ‰‹ç»Ÿè®¡åˆ—è¡¨ (å¯åŒ…å«å¤šå¤©æ•°æ®)
        editor_stats: å‰ªè¾‘å¸ˆç»Ÿè®¡åˆ—è¡¨
        start_date: å‘¨å¼€å§‹æ—¥æœŸ
        end_date: å‘¨ç»“æŸæ—¥æœŸ
        min_spend_threshold: å‚ä¸æœ€ä½³è¯„é€‰çš„æœ€ä½æ¶ˆè€—é˜ˆå€¼

    Returns:
        {
            'period': {start_date, end_date},
            'optimizer_summary': æŠ•æ‰‹å‘¨æ±‡æ€»,
            'editor_summary': å‰ªè¾‘å¸ˆå‘¨æ±‡æ€»,
            'best_optimizer': æœ€ä½³æŠ•æ‰‹,
            'best_editor': æœ€ä½³å‰ªè¾‘å¸ˆ
        }
    """
    print(f"[å‘¨æŠ¥] ç”Ÿæˆå‘¨æŠ¥æ±‡æ€»: {start_date} ~ {end_date}")

    def merge_weekly_stats(stats: List[Dict]) -> List[Dict]:
        """åˆå¹¶ä¸€å‘¨æ•°æ® (æŒ‰äººåèšåˆ)"""
        merged = {}
        for s in stats:
            name = s.get('name')
            if not name:
                continue
            if name not in merged:
                merged[name] = {
                    'name': name,
                    'meta_spend': 0, 'meta_revenue': 0,
                    'tt_spend': 0, 'tt_revenue': 0,
                    'all_spend': 0, 'all_revenue': 0,
                }
            channel = s.get('channel', '')
            spend = float(s.get('total_cost', 0))
            revenue = float(s.get('total_revenue', 0))

            if channel == 'facebook':
                merged[name]['meta_spend'] += spend
                merged[name]['meta_revenue'] += revenue
            elif channel == 'tiktok':
                merged[name]['tt_spend'] += spend
                merged[name]['tt_revenue'] += revenue
            elif channel == 'all':
                merged[name]['all_spend'] += spend
                merged[name]['all_revenue'] += revenue

        result = []
        for name, d in merged.items():
            total_spend = d['meta_spend'] + d['tt_spend'] + d['all_spend']
            total_revenue = d['meta_revenue'] + d['tt_revenue'] + d['all_revenue']
            result.append({
                'name': name,
                'meta_spend': d['meta_spend'],
                'meta_revenue': d['meta_revenue'],
                'meta_roas': d['meta_revenue'] / d['meta_spend'] if d['meta_spend'] > 0 else 0,
                'tt_spend': d['tt_spend'],
                'tt_revenue': d['tt_revenue'],
                'tt_roas': d['tt_revenue'] / d['tt_spend'] if d['tt_spend'] > 0 else 0,
                'total_spend': total_spend,
                'total_revenue': total_revenue,
                'total_roas': total_revenue / total_spend if total_spend > 0 else 0,
            })
        result.sort(key=lambda x: x['total_spend'], reverse=True)
        return result

    def find_best_performer(stats: List[Dict], threshold: float) -> Optional[Dict]:
        """æ‰¾å‡ºæœ€ä½³è¡¨ç°è€… (ç»¼åˆè¯„åˆ†)"""
        qualified = [s for s in stats if s['total_spend'] >= threshold]
        if not qualified:
            return None

        # æŒ‰ Spend æ’å
        spend_sorted = sorted(qualified, key=lambda x: x['total_spend'], reverse=True)
        for i, s in enumerate(spend_sorted):
            s['spend_rank'] = i + 1

        # æŒ‰ ROAS æ’å
        roas_sorted = sorted(qualified, key=lambda x: x['total_roas'], reverse=True)
        for i, s in enumerate(roas_sorted):
            s['roas_rank'] = i + 1

        # ç»¼åˆè¯„åˆ† = Spendæ’å + ROASæ’å (è¶Šå°è¶Šå¥½)
        for s in qualified:
            s['combined_score'] = s['spend_rank'] + s['roas_rank']

        # æ‰¾ç»¼åˆè¯„åˆ†æœ€ä½çš„
        best = min(qualified, key=lambda x: x['combined_score'])
        return best

    # åˆå¹¶å‘¨æ•°æ®
    opt_weekly = merge_weekly_stats(optimizer_stats)
    editor_weekly = merge_weekly_stats(editor_stats)

    # æ‰¾æœ€ä½³è¡¨ç°è€…
    best_opt = find_best_performer(opt_weekly, min_spend_threshold)
    best_editor = find_best_performer(editor_weekly, min_spend_threshold)

    # è®¡ç®—æ±‡æ€»
    opt_total_spend = sum(s['total_spend'] for s in opt_weekly)
    opt_total_revenue = sum(s['total_revenue'] for s in opt_weekly)
    editor_total_spend = sum(s['total_spend'] for s in editor_weekly)
    editor_total_revenue = sum(s['total_revenue'] for s in editor_weekly)

    result = {
        'period': {'start_date': start_date, 'end_date': end_date},
        'optimizer_summary': {
            'count': len(opt_weekly),
            'total_spend': opt_total_spend,
            'total_revenue': opt_total_revenue,
            'avg_roas': opt_total_revenue / opt_total_spend if opt_total_spend > 0 else 0,
            'details': opt_weekly
        },
        'editor_summary': {
            'count': len(editor_weekly),
            'total_spend': editor_total_spend,
            'total_revenue': editor_total_revenue,
            'avg_roas': editor_total_revenue / editor_total_spend if editor_total_spend > 0 else 0,
            'details': editor_weekly
        },
        'best_optimizer': best_opt,
        'best_editor': best_editor
    }

    # æ‰“å°å‘¨æŠ¥
    print(f"\n{'='*70}")
    print(f"å‘¨æŠ¥æ±‡æ€» ({start_date} ~ {end_date})")
    print(f"{'='*70}")

    print(f"\næŠ•æ‰‹å‘¨æ±‡æ€» ({len(opt_weekly)} äºº):")
    print(f"  æ€»æ¶ˆè€—: ${opt_total_spend:,.0f}")
    print(f"  æ€»æ”¶å…¥: ${opt_total_revenue:,.0f}")
    print(f"  å¹³å‡ ROAS: {opt_total_revenue/opt_total_spend*100:.1f}%" if opt_total_spend > 0 else "")

    if best_opt:
        print(f"\n  æœ€ä½³æŠ•æ‰‹: {best_opt['name']}")
        print(f"    Spend: ${best_opt['total_spend']:,.0f} (æ’å #{best_opt['spend_rank']})")
        print(f"    ROAS: {best_opt['total_roas']*100:.1f}% (æ’å #{best_opt['roas_rank']})")
        print(f"    ç»¼åˆè¯„åˆ†: {best_opt['combined_score']}")

    print(f"\nå‰ªè¾‘å¸ˆå‘¨æ±‡æ€» ({len(editor_weekly)} äºº):")
    print(f"  æ€»æ¶ˆè€—: ${editor_total_spend:,.0f}")
    print(f"  æ€»æ”¶å…¥: ${editor_total_revenue:,.0f}")
    print(f"  å¹³å‡ ROAS: {editor_total_revenue/editor_total_spend*100:.1f}%" if editor_total_spend > 0 else "")

    if best_editor:
        print(f"\n  æœ€ä½³å‰ªè¾‘å¸ˆ: {best_editor['name']}")
        print(f"    Spend: ${best_editor['total_spend']:,.0f} (æ’å #{best_editor['spend_rank']})")
        print(f"    ROAS: {best_editor['total_roas']*100:.1f}% (æ’å #{best_editor['roas_rank']})")
        print(f"    ç»¼åˆè¯„åˆ†: {best_editor['combined_score']}")

    print(f"{'='*70}\n")

    return result


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='XMP å¤šæ¸ é“æ•°æ®æŠ“å–')
    parser.add_argument('--date', help='æ—¥æœŸ YYYY-MM-DDï¼Œé»˜è®¤ä»Šå¤©')
    parser.add_argument('--yesterday', action='store_true', help='é‡‡é›†æ˜¨å¤© (T-1) çš„æ•°æ®')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° BigQuery')
    parser.add_argument('--stats', action='store_true', help='ç”ŸæˆæŠ•æ‰‹/å‰ªè¾‘å¸ˆç»Ÿè®¡')
    parser.add_argument('--excel', action='store_true', help='å¯¼å‡º Excel æ–‡ä»¶')
    parser.add_argument('--lark-doc', action='store_true', help='å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£')
    parser.add_argument('--from-bq', action='store_true', help='ä» BigQuery è¯»å–æ•°æ®ï¼ˆä¸é‡æ–°æŠ“å–ï¼‰')
    parser.add_argument('--doc-token', help='é£ä¹¦æ–‡æ¡£ tokenï¼Œé»˜è®¤ä» XMP_DOC_TOKEN ç¯å¢ƒå˜é‡è·å–')
    parser.add_argument('--weekly', action='store_true', help='ç”Ÿæˆå‘¨æŠ¥æ±‡æ€»')
    parser.add_argument('--days', type=int, default=7, help='å‘¨æŠ¥å¤©æ•°ï¼Œé»˜è®¤7å¤©')
    parser.add_argument('--relogin', action='store_true', help='å¼ºåˆ¶é‡æ–°ç™»å½•è·å– Token')
    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº† --reloginï¼Œåˆ é™¤ token æ–‡ä»¶å¼ºåˆ¶é‡æ–°ç™»å½•
    if args.relogin:
        if os.path.exists(XMP_TOKEN_FILE):
            os.remove(XMP_TOKEN_FILE)
            print(f"[XMP] å·²åˆ é™¤æ—§ Token æ–‡ä»¶ï¼Œå°†é‡æ–°ç™»å½•")
        else:
            print(f"[XMP] Token æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è¿›è¡Œç™»å½•")

    # ç¡®å®šé‡‡é›†æ—¥æœŸ
    # ä¼˜å…ˆçº§: --date > --yesterday > FETCH_YESTERDAY ç¯å¢ƒå˜é‡ > ä»Šå¤©
    if args.date:
        date_str = args.date
    elif args.yesterday or os.getenv('FETCH_YESTERDAY', '').lower() == 'true':
        date_str = (datetime.now(BEIJING_TZ) - timedelta(days=1)).strftime('%Y-%m-%d')
        print(f"[XMP] é‡‡é›† T-1 æ—¥æ•°æ®: {date_str}")
    else:
        date_str = datetime.now(BEIJING_TZ).strftime('%Y-%m-%d')

    # å‘¨æŠ¥æ¨¡å¼
    if args.weekly:
        end_date = datetime.strptime(date_str, '%Y-%m-%d')

        # ä½¿ç”¨ --days å‚æ•°è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆæ”¯æŒ7å¤©å®Œæ•´å‘¨æŠ¥ï¼‰
        days_count = args.days
        start_date = end_date - timedelta(days=days_count - 1)
        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')

        # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨ï¼ˆåŒ…æ‹¬å‘¨æœ«ï¼‰
        date_list = []
        current = start_date
        while current <= end_date:
            date_list.append(current.strftime('%Y-%m-%d'))
            current += timedelta(days=1)

        print(f"[å‘¨æŠ¥] ç»Ÿè®¡å‘¨æœŸ: {start_str} ~ {end_str} ({days_count}å¤©)")
        print(f"[å‘¨æŠ¥] æ—¥æœŸåˆ—è¡¨: {', '.join(date_list)}")

        # ä» BigQuery è¯»å–æ•°æ®ï¼ˆ--from-bq æ¨¡å¼ï¼‰
        if getattr(args, 'from_bq', False):
            print(f"[å‘¨æŠ¥] ä» BigQuery è¯»å–æ•°æ®...")
            bq_data = query_weekly_stats_from_bq(start_str, end_str)
            all_opt_stats = bq_data.get('optimizer_stats', [])
            all_editor_stats = bq_data.get('editor_stats', [])
            invalid_dates = bq_data.get('invalid_dates', [])

            if invalid_dates:
                print(f"\n[å‘¨æŠ¥] âš ï¸ ä»¥ä¸‹æ—¥æœŸæ•°æ®æ ¡éªŒå¤±è´¥: {invalid_dates}")

            if not all_opt_stats and not all_editor_stats:
                print(f"[å‘¨æŠ¥] æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
                return
        else:
            # é€å¤©æŠ“å–æ•°æ®ï¼ˆåŒ…æ‹¬å‘¨æœ«ï¼‰
            all_opt_stats = []
            all_editor_stats = []

            for day_str in date_list:
                print(f"\n--- æŠ“å– {day_str} ---")

                result = await run_with_stats(day_str, upload_bq=args.upload)
                all_opt_stats.extend(result.get('optimizer_stats', []))
                all_editor_stats.extend(result.get('editor_stats', []))

        # ç”Ÿæˆå‘¨æŠ¥æ±‡æ€»
        weekly = generate_weekly_summary(
            all_opt_stats, all_editor_stats,
            start_str, end_str
        )

        # å¯¼å‡º Excel
        if args.excel:
            export_stats_to_excel(all_opt_stats, all_editor_stats, f"{start_str}_to_{end_str}")

        # å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£
        if getattr(args, 'lark_doc', False):
            doc_token = getattr(args, 'doc_token', None)
            export_stats_to_lark_doc(all_opt_stats, all_editor_stats, f"{start_str}_to_{end_str}", doc_token)

        # ä¿å­˜å‘¨æŠ¥
        output_file = f"xmp_weekly_{start_str}_to_{end_str}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(weekly, f, ensure_ascii=False, indent=2, default=str)
        print(f"å‘¨æŠ¥å·²ä¿å­˜åˆ°: {output_file}")

    # ä» BigQuery è¯»å–æ•°æ®ç”Ÿæˆæ—¥æŠ¥ï¼ˆä¸æŠ“å–ï¼‰
    elif getattr(args, 'from_bq', False):
        print(f"[BQ] ä» BigQuery è¯»å– {date_str} çš„æ•°æ®...")
        bq_data = query_stats_from_bq(date_str)
        opt_stats = bq_data.get('optimizer_stats', [])
        editor_stats = bq_data.get('editor_stats', [])
        batch_id = bq_data.get('batch_id')
        batch_valid = bq_data.get('batch_valid', True)

        if not opt_stats and not editor_stats:
            print(f"[BQ] æœªæ‰¾åˆ° {date_str} çš„æ•°æ®")
            return

        # æ ¡éªŒ batch æ—¶æ•ˆæ€§
        if not batch_valid:
            print(f"[BQ] âš ï¸ æ•°æ®æ—¶æ•ˆæ€§æ ¡éªŒå¤±è´¥ï¼Œè·³è¿‡æ—¥æŠ¥ç”Ÿæˆ")
            print(f"[BQ]   batch_id={batch_id} ä¸æ˜¯ {date_str} ä¹‹åæŠ“å–çš„")
            print(f"[BQ]   è¯·æ£€æŸ¥å‡Œæ™¨2ç‚¹çš„ T-1 æ•°æ®é‡‡é›†ä»»åŠ¡æ˜¯å¦æ­£å¸¸æ‰§è¡Œ")
            return

        # å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£
        if getattr(args, 'lark_doc', False):
            doc_token = getattr(args, 'doc_token', None)
            export_stats_to_lark_doc(opt_stats, editor_stats, date_str, doc_token)

        # å¯¼å‡º Excel
        if args.excel:
            export_stats_to_excel(opt_stats, editor_stats, date_str)

        print(f"[å®Œæˆ] æ—¥æŠ¥ç”Ÿæˆå®Œæ¯•: {date_str}")

    elif args.stats or args.excel or getattr(args, 'lark_doc', False):
        result = await run_with_stats(date_str, upload_bq=args.upload)
        if args.excel and result.get('optimizer_stats'):
            export_stats_to_excel(
                result.get('optimizer_stats', []),
                result.get('editor_stats', []),
                date_str
            )
        # å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£
        if getattr(args, 'lark_doc', False) and result.get('optimizer_stats'):
            doc_token = getattr(args, 'doc_token', None)
            export_stats_to_lark_doc(
                result.get('optimizer_stats', []),
                result.get('editor_stats', []),
                date_str,
                doc_token
            )
        output_file = f"xmp_data_{date_str.replace('-', '')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

    else:
        result = await run_once(date_str, upload_bq=args.upload)
        output_file = f"xmp_data_{date_str.replace('-', '')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")


if __name__ == '__main__':
    asyncio.run(main())


def test_material_api():
    """æµ‹è¯•ç´ æè¯¦æƒ… API"""
    from datetime import datetime

    scraper = XMPEditorStatsScraper()

    # æµ‹è¯•1: æŒ‰æ—¥æœŸæŸ¥è¯¢
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"\n=== æµ‹è¯•1: æŸ¥è¯¢ {today} çš„ç´ æ ===")
    materials = scraper.fetch_material_details(
        start_date=today,
        end_date=today,
        page_size=10
    )

    if materials:
        print(f"è·å–åˆ° {len(materials)} ä¸ªç´ æ")
        for m in materials[:3]:
            designer = m.get('designer', [])
            name = designer[0].get('name') if designer else 'æœªçŸ¥'
            print(f"  - {m.get('material_name')[:30]}... | å‰ªè¾‘å¸ˆ: {name}")

    # æµ‹è¯•2: æ—¥äº§å‡ºç»Ÿè®¡
    print(f"\n=== æµ‹è¯•2: å‰ªè¾‘å¸ˆæ—¥äº§å‡ºç»Ÿè®¡ ===")
    output = scraper.fetch_daily_editor_output(today)
    print(f"æ€»ç´ ææ•°: {output['total_materials']}")
    for editor in output['editors'][:5]:
        print(f"  - {editor['name']}: {editor['count']} ä¸ªç´ æ")


def test_editor_performance():
    """æµ‹è¯•å‰ªè¾‘å¸ˆäº§å‡ºä¸è´¨é‡æŠ¥è¡¨"""
    from datetime import datetime, timedelta

    scraper = XMPEditorStatsScraper()

    # è·å–æœ€è¿‘7å¤©çš„æ•°æ®
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')

    print(f"\n=== æµ‹è¯•: å‰ªè¾‘å¸ˆäº§å‡ºä¸è´¨é‡æŠ¥è¡¨ ===")
    print(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")

    result = scraper.fetch_editor_performance(
        start_date=start_date,
        end_date=end_date,
        hot_threshold=500.0,
        roas_threshold=0.45
    )

    if result['editors']:
        print(f"\nå‰5ä½å‰ªè¾‘å¸ˆè¯¦æƒ…:")
        for e in result['editors'][:5]:
            print(f"  {e['name']}:")
            print(f"    ç´ ææ•°: {e['material_count']}")
            print(f"    æ¶ˆè€—: ${e['total_cost']:,.2f}")
            print(f"    D0 ROAS: {e['d0_roas']*100:.1f}%")
            print(f"    çˆ†æ¬¾ç‡: {e['hot_rate']*100:.1f}%")
            print(f"    Topç´ æ: {e['top_material']}")


if __name__ == '__test__':
    test_material_api()