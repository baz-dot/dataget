"""
DataEye çŸ­å‰§å¸‚åœºæ½œåŠ›æ•°æ®é‡‡é›†å™¨
ä» DataEye å¹³å°æŠ“å–çŸ­å‰§å¹¿å‘ŠæŠ•æ”¾æ•°æ®
"""

import os
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from playwright.sync_api import sync_playwright, Page, Browser, TimeoutError as PlaywrightTimeout

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('dataeye_scraper.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class DataEyeScraper:
    """DataEye çŸ­å‰§æ•°æ®é‡‡é›†å™¨"""

    def __init__(self, headless: bool = False, cookie_file: str = 'dataeye_cookies.json'):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            cookie_file: Cookie ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        self.headless = headless
        self.cookie_file = cookie_file
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None

        # DataEye å¹³å°é…ç½®
        self.base_url = 'https://oversea-v2.dataeye.com'
        self.target_url = 'https://oversea-v2.dataeye.com/playlet/playlet-material'
        self.username = os.getenv('DATAEYE_USERNAME')
        self.password = os.getenv('DATAEYE_PASSWORD')

        # å‰§ç›®æœç´¢é…ç½® - ä½¿ç”¨å‰§ç›®åç§°æœç´¢æ¡†è€Œä¸æ˜¯ç´ ææœç´¢æ¡†
        self.use_drama_name_search = True

        logger.info("åˆå§‹åŒ– DataEye é‡‡é›†å™¨")
        logger.info(f"ç›®æ ‡åœ°å€: {self.target_url}")
        logger.info(f"æ— å¤´æ¨¡å¼: {headless}")

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.close()

    def start(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        logger.info("å¯åŠ¨æµè§ˆå™¨...")
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=['--start-maximized']
        )
        context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        self.page = context.new_page()
        logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            self.page.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        logger.info("æµè§ˆå™¨å·²å…³é—­")

    def load_cookies(self) -> bool:
        """åŠ è½½ä¿å­˜çš„ Cookie"""
        if not os.path.exists(self.cookie_file):
            logger.info("Cookie æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°ç™»å½•")
            return False

        try:
            with open(self.cookie_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            self.page.context.add_cookies(cookies)
            logger.info(f"æˆåŠŸåŠ è½½ {len(cookies)} ä¸ª Cookie")
            return True
        except Exception as e:
            logger.error(f"åŠ è½½ Cookie å¤±è´¥: {e}")
            return False

    def save_cookies(self):
        """ä¿å­˜å½“å‰ Cookie"""
        try:
            cookies = self.page.context.cookies()
            with open(self.cookie_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, indent=2, ensure_ascii=False)
            logger.info(f"æˆåŠŸä¿å­˜ {len(cookies)} ä¸ª Cookie")
        except Exception as e:
            logger.error(f"ä¿å­˜ Cookie å¤±è´¥: {e}")

    def login(self) -> bool:
        """ç™»å½• DataEye å¹³å°"""
        logger.info("å¼€å§‹ç™»å½•æµç¨‹...")

        # å°è¯•åŠ è½½ Cookie
        if self.load_cookies():
            logger.info("å°è¯•ä½¿ç”¨ Cookie ç™»å½•...")
            self.page.goto(self.target_url, timeout=60000)
            time.sleep(3)

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸç™»å½•
            if self._check_login_status():
                logger.info("âœ“ Cookie ç™»å½•æˆåŠŸ")
                return True
            else:
                logger.warning("Cookie å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")

        # Cookie ç™»å½•å¤±è´¥ï¼Œä½¿ç”¨è´¦å·å¯†ç ç™»å½•
        if not self.username or not self.password:
            logger.error("æœªé…ç½® DATAEYE_USERNAME æˆ– DATAEYE_PASSWORD")
            logger.error("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®è´¦å·å¯†ç ï¼Œæˆ–æ‰‹åŠ¨ä¿å­˜æœ‰æ•ˆçš„ Cookie")
            return False

        logger.info("ä½¿ç”¨è´¦å·å¯†ç ç™»å½•...")
        return self._login_with_credentials()

    def _check_login_status(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨ç™»å½•é¡µé¢
            if 'login' in self.page.url:
                return False

            # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æœç´¢æ¡†ï¼ˆç™»å½•åæ‰æœ‰ï¼‰
            search_input = self.page.query_selector('input[placeholder*="æœç´¢"]')
            return search_input is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False

    def _login_with_credentials(self) -> bool:
        """ä½¿ç”¨è´¦å·å¯†ç ç™»å½•"""
        try:
            # è®¿é—®ç™»å½•é¡µé¢
            login_url = f"{self.base_url}/login"
            logger.info(f"è®¿é—®ç™»å½•é¡µé¢: {login_url}")
            self.page.goto(login_url, timeout=60000)
            time.sleep(2)

            # è¾“å…¥ç”¨æˆ·å
            logger.info("è¾“å…¥ç”¨æˆ·å...")
            username_input = self.page.wait_for_selector('input[type="text"]', timeout=10000)
            username_input.fill(self.username)
            time.sleep(1)

            # è¾“å…¥å¯†ç 
            logger.info("è¾“å…¥å¯†ç ...")
            password_input = self.page.wait_for_selector('input[type="password"]', timeout=10000)
            password_input.fill(self.password)
            time.sleep(1)

            # ç‚¹å‡»ç™»å½•æŒ‰é’®
            logger.info("ç‚¹å‡»ç™»å½•æŒ‰é’®...")
            login_button = self.page.wait_for_selector('button[type="submit"]', timeout=10000)
            login_button.click()

            # ç­‰å¾…ç™»å½•å®Œæˆ
            logger.info("ç­‰å¾…ç™»å½•å®Œæˆ...")
            time.sleep(5)

            # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
            if self._check_login_status():
                logger.info("âœ“ è´¦å·å¯†ç ç™»å½•æˆåŠŸ")
                self.save_cookies()
                return True
            else:
                logger.error("âœ— ç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è´¦å·å¯†ç ")
                return False

        except Exception as e:
            logger.error(f"ç™»å½•è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def search_drama(self, drama_name: str) -> Dict:
        """
        æœç´¢çŸ­å‰§æ•°æ® - åˆ†ä¸¤æ¬¡è·å–: è¿‘2å¹´æ•°æ® + è¿‘30å¤©æ•°æ®

        Args:
            drama_name: çŸ­å‰§åç§°

        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"å¼€å§‹æœç´¢çŸ­å‰§: {drama_name}")
        logger.info(f"{'='*60}")

        try:
            # 1. è®¿é—®ç›®æ ‡é¡µé¢
            logger.info("æ­¥éª¤ 1: è®¿é—®çŸ­å‰§ç´ æé¡µé¢...")
            self.page.goto(self.target_url, timeout=60000)
            time.sleep(3)

            # 2. ç‚¹å‡»"æµ·å¤–çŸ­å‰§ç‰ˆæœ¬"
            logger.info("æ­¥éª¤ 2: ç‚¹å‡»æµ·å¤–çŸ­å‰§ç‰ˆæœ¬...")
            if not self._click_overseas_version():
                logger.error("âœ— ç‚¹å‡»æµ·å¤–çŸ­å‰§ç‰ˆæœ¬å¤±è´¥")
                return {"success": False, "error": "æ— æ³•ç‚¹å‡»æµ·å¤–çŸ­å‰§ç‰ˆæœ¬"}

            # 3. è¾“å…¥æœç´¢å…³é”®è¯
            logger.info("æ­¥éª¤ 3: è¾“å…¥æœç´¢å…³é”®è¯...")
            if not self._input_search_keyword(drama_name):
                logger.error("âœ— è¾“å…¥æœç´¢å…³é”®è¯å¤±è´¥")
                return {"success": False, "error": "æ— æ³•è¾“å…¥æœç´¢å…³é”®è¯"}

            # 4. è®¾ç½®ç›‘å¬å™¨å¹¶ç‚¹å‡»æœç´¢
            logger.info("æ­¥éª¤ 4: è®¾ç½®ç›‘å¬å™¨å¹¶ç‚¹å‡»æœç´¢...")
            data_2y = self._capture_api_with_action(
                drama_name, "2å¹´",
                lambda: self._click_search_button()
            )

            # 5. ç‚¹å‡»"è¿‘30å¤©"ç­›é€‰
            logger.info("æ­¥éª¤ 5: ç‚¹å‡»è¿‘30å¤©ç­›é€‰...")
            if not self._click_time_filter_30d():
                logger.warning("âœ— ç‚¹å‡»è¿‘30å¤©ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®")
                data_30d = None
            else:
                # 6. è®¾ç½®ç›‘å¬å™¨å¹¶ç‚¹å‡»è¿‘30å¤©
                logger.info("æ­¥éª¤ 6: è·å–è¿‘30å¤©æ•°æ®...")
                data_30d = self._capture_api_with_action(
                    drama_name, "30å¤©",
                    lambda: time.sleep(0.5)  # ç‚¹å‡»å·²å®Œæˆï¼Œåªéœ€ç­‰å¾…
                )

            # 8. åˆå¹¶ä¸¤æ¬¡æ•°æ®
            return self._merge_time_window_data(drama_name, data_2y, data_30d)

        except Exception as e:
            logger.error(f"æœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
            return {"success": False, "error": str(e)}

    def _process_api_response(self, drama_name: str, api_data: Dict) -> Dict:
        """
        å¤„ç† searchCreative API è¿”å›çš„æ•°æ®ï¼Œå¹¶æŒ‰æ—¶é—´çª—å£èšåˆ

        Args:
            drama_name: çŸ­å‰§åç§°
            api_data: API è¿”å›çš„ JSON æ•°æ®

        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        try:
            logger.info("å¼€å§‹å¤„ç† API å“åº”æ•°æ®...")

            # æå–åŸºæœ¬ä¿¡æ¯
            status_code = api_data.get('statusCode', 0)
            msg = api_data.get('msg', '')

            if status_code != 200:
                logger.error(f"API è¿”å›é”™è¯¯: {msg}")
                return {
                    "success": False,
                    "drama_name": drama_name,
                    "error": f"API é”™è¯¯: {msg}"
                }

            # æå–åˆ†é¡µä¿¡æ¯
            page_info = api_data.get('page', {})
            total_records = page_info.get('totalRecords', 0)

            # æå–ç´ æåˆ—è¡¨
            content = api_data.get('content', {})
            search_list = content.get('searchList', [])

            logger.info(f"âœ“ æ‰¾åˆ° {len(search_list)} æ¡ç´ æï¼Œæ€»è®°å½•æ•°: {total_records}")

            # ä¿å­˜åŸå§‹ API æ•°æ®
            screenshot_path = f"screenshots/api_{drama_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs('screenshots', exist_ok=True)
            with open(screenshot_path, 'w', encoding='utf-8') as f:
                json.dump(api_data, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ“ å·²ä¿å­˜åŸå§‹ API æ•°æ®: {screenshot_path}")

            # æŒ‰æ—¶é—´çª—å£èšåˆæ•°æ®
            aggregated_data = self._aggregate_by_time_windows(drama_name, search_list)

            return {
                "success": True,
                "drama_name": drama_name,
                "total_records": total_records,
                "materials": search_list,
                "page_info": page_info,
                "raw_api_file": screenshot_path,
                "aggregated": aggregated_data
            }

        except Exception as e:
            logger.error(f"å¤„ç† API å“åº”å¤±è´¥: {e}")
            return {
                "success": False,
                "drama_name": drama_name,
                "error": str(e)
            }

    def _aggregate_by_time_windows(self, drama_name: str, materials: List[Dict]) -> Dict:
        """
        æŒ‰æ—¶é—´çª—å£èšåˆç´ ææ•°æ®

        çª—å£ A: è¿‘ 30 å¤©ï¼ˆæœ€è¿‘çƒ­åº¦ï¼‰
        çª—å£ B: è¿‘ 2 å¹´ï¼ˆç”Ÿå‘½å‘¨æœŸï¼‰

        Args:
            drama_name: çŸ­å‰§åç§°
            materials: ç´ æåˆ—è¡¨

        Returns:
            èšåˆåçš„æ•°æ®
        """
        from datetime import datetime, timedelta

        logger.info(f"å¼€å§‹æŒ‰æ—¶é—´çª—å£èšåˆæ•°æ®...")

        # è®¡ç®—æ—¶é—´çª—å£
        today = datetime.now()
        window_30d = today - timedelta(days=30)
        window_2y = today - timedelta(days=730)

        # åˆå§‹åŒ–èšåˆæ•°æ®
        data_30d = {
            "materials": [],
            "total_exposure": 0,
            "total_downloads": 0,
            "creative_count": 0,
            "countries": set(),
            "platforms": set(),
            "publishers": set()
        }

        data_2y = {
            "materials": [],
            "total_exposure": 0,
            "total_downloads": 0,
            "creative_count": 0,
            "countries": set(),
            "platforms": set(),
            "publishers": set(),
            "first_seen": None,
            "last_seen": None,
            "active_days": set()
        }

        # éå†ç´ æè¿›è¡Œèšåˆ
        for material in materials:
            try:
                # è§£ææ—¥æœŸ
                first_seen_str = material.get('firstSeen', '')
                last_seen_str = material.get('lastSeen', '')

                if not first_seen_str or not last_seen_str:
                    continue

                first_seen = datetime.strptime(first_seen_str, '%Y-%m-%d')
                last_seen = datetime.strptime(last_seen_str, '%Y-%m-%d')

                # æå–åŸºæœ¬æ•°æ®
                exposure = material.get('exposureNum', 0)
                downloads = material.get('downloadNum', 0)

                # çª—å£ B: è¿‘ 2 å¹´æ•°æ®
                if first_seen >= window_2y or last_seen >= window_2y:
                    data_2y["materials"].append(material)
                    data_2y["total_exposure"] += exposure
                    data_2y["total_downloads"] += downloads
                    data_2y["creative_count"] += 1

                    # æ›´æ–°é¦–æ¬¡/æœ€åæŠ•æ”¾æ—¥æœŸ
                    if data_2y["first_seen"] is None or first_seen < data_2y["first_seen"]:
                        data_2y["first_seen"] = first_seen
                    if data_2y["last_seen"] is None or last_seen > data_2y["last_seen"]:
                        data_2y["last_seen"] = last_seen

                    # ç»Ÿè®¡æ´»è·ƒå¤©æ•°
                    release_days = material.get('releaseDay', 0)
                    for i in range(release_days):
                        day = first_seen + timedelta(days=i)
                        data_2y["active_days"].add(day.strftime('%Y-%m-%d'))

                    # ç»Ÿè®¡å›½å®¶
                    countries = material.get('countries', [])
                    for country in countries:
                        data_2y["countries"].add(country.get('countryName', ''))

                    # ç»Ÿè®¡å¹³å°
                    media = material.get('media', {})
                    if media:
                        data_2y["platforms"].add(media.get('mediaName', ''))

                    # ç»Ÿè®¡åˆ¶ä½œæ–¹
                    publisher = material.get('publisher', {})
                    if publisher:
                        data_2y["publishers"].add(publisher.get('publisherName', ''))

                # çª—å£ A: è¿‘ 30 å¤©æ•°æ®
                if last_seen >= window_30d:
                    data_30d["materials"].append(material)
                    data_30d["total_exposure"] += exposure
                    data_30d["total_downloads"] += downloads
                    data_30d["creative_count"] += 1

                    # ç»Ÿè®¡å›½å®¶
                    countries = material.get('countries', [])
                    for country in countries:
                        data_30d["countries"].add(country.get('countryName', ''))

                    # ç»Ÿè®¡å¹³å°
                    media = material.get('media', {})
                    if media:
                        data_30d["platforms"].add(media.get('mediaName', ''))

                    # ç»Ÿè®¡åˆ¶ä½œæ–¹
                    publisher = material.get('publisher', {})
                    if publisher:
                        data_30d["publishers"].add(publisher.get('publisherName', ''))

            except Exception as e:
                logger.debug(f"å¤„ç†ç´ æå¤±è´¥: {e}")
                continue

        # è½¬æ¢ set ä¸º list
        data_30d["countries"] = list(data_30d["countries"])
        data_30d["platforms"] = list(data_30d["platforms"])
        data_30d["publishers"] = list(data_30d["publishers"])

        data_2y["countries"] = list(data_2y["countries"])
        data_2y["platforms"] = list(data_2y["platforms"])
        data_2y["publishers"] = list(data_2y["publishers"])
        data_2y["active_days_count"] = len(data_2y["active_days"])
        data_2y["active_days"] = list(data_2y["active_days"])

        # è®¡ç®—ç”Ÿå‘½å‘¨æœŸ
        if data_2y["first_seen"] and data_2y["last_seen"]:
            lifecycle_days = (data_2y["last_seen"] - data_2y["first_seen"]).days
            data_2y["lifecycle_days"] = lifecycle_days
            data_2y["first_seen"] = data_2y["first_seen"].strftime('%Y-%m-%d')
            data_2y["last_seen"] = data_2y["last_seen"].strftime('%Y-%m-%d')

        logger.info(f"âœ“ è¿‘30å¤©: {data_30d['creative_count']} æ¡ç´ æ, æ›å…‰ {data_30d['total_exposure']:,}")
        logger.info(f"âœ“ è¿‘2å¹´: {data_2y['creative_count']} æ¡ç´ æ, æ›å…‰ {data_2y['total_exposure']:,}, ç”Ÿå‘½å‘¨æœŸ {data_2y.get('lifecycle_days', 0)} å¤©")

        return {
            "window_30d": data_30d,
            "window_2y": data_2y
        }

    def _click_overseas_version(self) -> bool:
        """ç‚¹å‡»æµ·å¤–çŸ­å‰§ç‰ˆæœ¬"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾"æµ·å¤–çŸ­å‰§ç‰ˆæœ¬"æŒ‰é’®
            selectors = [
                'text="æµ·å¤–çŸ­å‰§ç‰ˆæœ¬"',
                'button:has-text("æµ·å¤–çŸ­å‰§ç‰ˆæœ¬")',
                'div:has-text("æµ·å¤–çŸ­å‰§ç‰ˆæœ¬")',
                '[class*="tab"]:has-text("æµ·å¤–çŸ­å‰§ç‰ˆæœ¬")',
            ]

            for selector in selectors:
                try:
                    element = self.page.wait_for_selector(selector, timeout=5000)
                    if element:
                        element.click()
                        logger.info("âœ“ æˆåŠŸç‚¹å‡»æµ·å¤–çŸ­å‰§ç‰ˆæœ¬")
                        time.sleep(2)
                        return True
                except:
                    continue

            logger.warning("æœªæ‰¾åˆ°æµ·å¤–çŸ­å‰§ç‰ˆæœ¬æŒ‰é’®ï¼Œå¯èƒ½å·²ç»åœ¨è¯¥ç‰ˆæœ¬")
            return True

        except Exception as e:
            logger.error(f"ç‚¹å‡»æµ·å¤–çŸ­å‰§ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

    def _input_search_keyword(self, keyword: str) -> bool:
        """è¾“å…¥æœç´¢å…³é”®è¯ - ä½¿ç”¨ç¬¬äºŒä¸ªæœç´¢æ¡†ï¼ˆå‰§ç›®åç§°æœç´¢æ¡†ï¼‰"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰æœç´¢è¾“å…¥æ¡†
            selectors = [
                'input[placeholder*="æœç´¢"]',
                'input[placeholder*="search"]',
                'input[type="text"]',
            ]

            for selector in selectors:
                try:
                    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„è¾“å…¥æ¡†
                    all_inputs = self.page.query_selector_all(selector)
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(all_inputs)} ä¸ªè¾“å…¥æ¡†")

                    if len(all_inputs) >= 2:
                        # ä½¿ç”¨ç¬¬äºŒä¸ªæœç´¢æ¡†ï¼ˆç´¢å¼•ä¸º1ï¼‰
                        search_input = all_inputs[1]

                        # è·å– placeholder ç¡®è®¤
                        placeholder = search_input.get_attribute('placeholder') or ''
                        logger.info(f"ä½¿ç”¨ç¬¬äºŒä¸ªè¾“å…¥æ¡†ï¼Œplaceholder: {placeholder}")

                        # æ¸…ç©ºè¾“å…¥æ¡†
                        search_input.click()
                        time.sleep(0.3)

                        # ä½¿ç”¨ fill æ–¹æ³•ç›´æ¥å¡«å……ï¼ˆæ›´å¯é ï¼‰
                        search_input.fill(keyword)
                        time.sleep(0.5)

                        # éªŒè¯è¾“å…¥å†…å®¹
                        input_value = search_input.input_value()
                        logger.info(f"âœ“ è¾“å…¥æ¡†å½“å‰å€¼: {input_value}")

                        # å¦‚æœ fill å¤±è´¥ï¼Œå°è¯• type æ–¹æ³•
                        if not input_value or input_value != keyword:
                            logger.warning(f"âš ï¸ fill æ–¹æ³•å¤±è´¥ï¼Œå°è¯• type æ–¹æ³•...")
                            search_input.click()
                            time.sleep(0.2)
                            search_input.press('Control+A')
                            time.sleep(0.1)
                            search_input.type(keyword, delay=50)
                            time.sleep(0.5)

                            # å†æ¬¡éªŒè¯
                            input_value = search_input.input_value()
                            logger.info(f"âœ“ é‡æ–°éªŒè¯è¾“å…¥æ¡†å€¼: {input_value}")

                        if input_value and keyword in input_value:
                            logger.info(f"âœ“ æˆåŠŸè¾“å…¥å…³é”®è¯: {keyword}")
                        else:
                            logger.warning(f"âš ï¸ è¾“å…¥å€¼ä¸åŒ¹é…! æœŸæœ›: {keyword}, å®é™…: {input_value}")
                            # å³ä½¿ä¸åŒ¹é…ä¹Ÿç»§ç»­ï¼Œå› ä¸ºå¯èƒ½æ˜¯é¡µé¢æ˜¾ç¤ºé—®é¢˜

                        # ç­‰å¾…æœç´¢å»ºè®®å‡ºç°
                        time.sleep(1.5)

                        return True
                    elif len(all_inputs) == 1:
                        logger.warning(f"åªæ‰¾åˆ°1ä¸ªè¾“å…¥æ¡†ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé€‰æ‹©å™¨")
                        continue
                except Exception as e:
                    logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                    continue

            logger.error("æœªæ‰¾åˆ°ç¬¬äºŒä¸ªæœç´¢è¾“å…¥æ¡†")
            return False

        except Exception as e:
            logger.error(f"è¾“å…¥æœç´¢å…³é”®è¯å¤±è´¥: {e}")
            return False

    def _click_search_button(self) -> bool:
        """ç‚¹å‡»æœç´¢æŒ‰é’®"""
        try:
            # æ–¹å¼1: å°è¯•æŸ¥æ‰¾å¹¶ç‚¹å‡»æœç´¢å›¾æ ‡æŒ‰é’®
            icon_selectors = [
                'button[class*="search"]',
                'span[class*="icon-search"]',
                'i[class*="search"]',
                '.ant-btn:has([class*="search"])',
            ]

            for selector in icon_selectors:
                try:
                    button = self.page.wait_for_selector(selector, timeout=2000)
                    if button and button.is_visible():
                        logger.info(f"æ‰¾åˆ°æœç´¢æŒ‰é’®: {selector}")
                        button.click()
                        logger.info("âœ“ æˆåŠŸç‚¹å‡»æœç´¢æŒ‰é’®")
                        time.sleep(2)  # ç­‰å¾…æœç´¢æ‰§è¡Œ
                        return True
                except:
                    continue

            # æ–¹å¼2: æŸ¥æ‰¾æ–‡æœ¬ä¸º"æœç´¢"çš„æŒ‰é’®
            text_selectors = [
                'button:has-text("æœç´¢")',
                'button:has-text("search")',
            ]

            for selector in text_selectors:
                try:
                    button = self.page.wait_for_selector(selector, timeout=2000)
                    if button:
                        button.click()
                        logger.info("âœ“ æˆåŠŸç‚¹å‡»æœç´¢æŒ‰é’®ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰")
                        time.sleep(2)
                        return True
                except:
                    continue

            # æ–¹å¼3: æŒ‰å›è½¦é”®è§¦å‘æœç´¢ï¼ˆæœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼‰
            logger.info("å°è¯•æŒ‰å›è½¦é”®è§¦å‘æœç´¢...")
            self.page.keyboard.press('Enter')
            logger.info("âœ“ å·²æŒ‰å›è½¦é”®")
            time.sleep(2)
            return True

        except Exception as e:
            logger.error(f"ç‚¹å‡»æœç´¢æŒ‰é’®å¤±è´¥: {e}")
            return False

    def _capture_api_with_action(self, drama_name: str, time_window: str, action_func, enable_pagination: bool = True) -> Optional[Dict]:
        """
        ä½¿ç”¨ expect_response æ•è· API å“åº”ï¼Œå¹¶è‡ªåŠ¨ç¿»é¡µè·å–æ‰€æœ‰æ•°æ®

        Args:
            drama_name: çŸ­å‰§åç§°
            time_window: æ—¶é—´çª—å£æ ‡è¯† (å¦‚ "2å¹´", "30å¤©")
            action_func: è§¦å‘ API è¯·æ±‚çš„åŠ¨ä½œå‡½æ•°
            enable_pagination: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç¿»é¡µ (é»˜è®¤: True)

        Returns:
            API å“åº”çš„ JSON æ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰é¡µé¢çš„æ•°æ®ï¼‰ï¼Œå¦‚æœæ•è·å¤±è´¥è¿”å› None
        """
        try:
            logger.info(f"âœ“ å‡†å¤‡æ•è· {time_window} API å“åº”...")

            # ä½¿ç”¨ expect_response ç­‰å¾…ç‰¹å®šçš„ API å“åº”
            with self.page.expect_response(
                lambda response: 'searchCreative' in response.url and response.status == 200,
                timeout=30000  # 30ç§’è¶…æ—¶
            ) as response_info:
                # æ‰§è¡Œè§¦å‘ API çš„åŠ¨ä½œ
                action_func()

            # è·å–å“åº”
            response = response_info.value
            logger.info(f"âœ“ æ•è·åˆ° API: {response.url}")

            # ä¿å­˜ç¬¬ä¸€é¡µçš„è¯·æ±‚å‚æ•°ä¾›åç»­ç¿»é¡µä½¿ç”¨
            try:
                request_body = response.request.post_data
                logger.debug(f"è¯·æ±‚ä½“ç±»å‹: {type(request_body)}")

                if request_body:
                    logger.debug(f"è¯·æ±‚ä½“å†…å®¹: {request_body}")

                    # å°è¯•è§£æ JSON
                    try:
                        self._last_search_params = json.loads(request_body)
                        logger.info(f"âœ“ å·²ä¿å­˜æœç´¢å‚æ•°(JSON): {json.dumps(self._last_search_params, ensure_ascii=False)}")
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯ JSONï¼Œå°è¯•è§£æ URL ç¼–ç å‚æ•°
                        logger.info("è¯·æ±‚ä½“ä¸æ˜¯ JSONï¼Œå°è¯•è§£æ URL ç¼–ç å‚æ•°...")
                        from urllib.parse import parse_qs
                        params_dict = {}
                        for pair in request_body.split('&'):
                            if '=' in pair:
                                key, value = pair.split('=', 1)
                                params_dict[key] = value
                        self._last_search_params = params_dict
                        logger.info(f"âœ“ å·²ä¿å­˜æœç´¢å‚æ•°(URLç¼–ç ): {json.dumps(self._last_search_params, ensure_ascii=False)}")
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆ
                    logger.warning("âš  è¯·æ±‚ä½“ä¸ºç©ºï¼Œä½¿ç”¨æœ€å°å‚æ•°é›†...")
                    self._last_search_params = {
                        "pageNum": 1,
                        "pageSize": 40,
                        "keyword": drama_name,
                    }
                    logger.warning(f"âš  ä½¿ç”¨æœ€å°å‚æ•°é›†: {json.dumps(self._last_search_params, ensure_ascii=False)}")

            except Exception as e:
                logger.error(f"âœ— ä¿å­˜æœç´¢å‚æ•°å¤±è´¥: {e}")
                logger.error(f"   åŸå§‹æ•°æ®: {request_body[:500] if request_body else 'None'}")
                self._last_search_params = {
                    "pageNum": 1,
                    "pageSize": 40,
                    "keyword": drama_name,
                }
                logger.warning(f"âš  ä½¿ç”¨æœ€å°å‚æ•°é›†: {json.dumps(self._last_search_params, ensure_ascii=False)}")

            # è§£æ JSON
            json_data = response.json()
            content = json_data.get('content', {})

            if isinstance(content, dict) and 'searchList' in content:
                # å¦‚æœå¯ç”¨ç¿»é¡µåŠŸèƒ½ï¼Œè‡ªåŠ¨è·å–æ‰€æœ‰é¡µé¢æ•°æ®
                if enable_pagination:
                    json_data = self._capture_all_pages(drama_name, time_window, json_data)
                else:
                    # ä¸å¯ç”¨ç¿»é¡µæ—¶ï¼Œä¿å­˜ç¬¬ä¸€é¡µæ•°æ®
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filepath = f"screenshots/api_{drama_name}_{time_window}_{timestamp}.json"
                    os.makedirs('screenshots', exist_ok=True)

                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(json_data, f, indent=2, ensure_ascii=False)
                    logger.info(f"âœ“ å·²ä¿å­˜: {filepath}")

                return json_data
            else:
                logger.warning(f"content ä¸ç¬¦åˆè¦æ±‚")
                return None

        except Exception as e:
            logger.error(f"æ•è·å¤±è´¥: {e}")
            return None

    def _capture_all_pages(self, drama_name: str, time_window: str, first_page_data: Dict) -> Dict:
        """
        è‡ªåŠ¨ç¿»é¡µè·å–æ‰€æœ‰æ•°æ®

        Args:
            drama_name: çŸ­å‰§åç§°
            time_window: æ—¶é—´çª—å£æ ‡è¯† (å¦‚ "2å¹´", "30å¤©")
            first_page_data: ç¬¬ä¸€é¡µçš„æ•°æ®

        Returns:
            åˆå¹¶æ‰€æœ‰é¡µé¢åçš„å®Œæ•´æ•°æ®
        """
        try:
            # æå–åˆ†é¡µä¿¡æ¯
            page_info = first_page_data.get('page', {})
            total_records = page_info.get('totalRecords', 0)
            page_size = page_info.get('pageSize', 40)

            # è®¡ç®—æ€»é¡µæ•°
            total_pages = (total_records + page_size - 1) // page_size

            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: æ€»è®°å½•={total_records}, æ¯é¡µ={page_size}, æ€»é¡µæ•°={total_pages}")

            # å¦‚æœåªæœ‰ä¸€é¡µï¼Œç›´æ¥è¿”å›
            if total_pages <= 1:
                logger.info("âœ“ åªæœ‰ä¸€é¡µæ•°æ®ï¼Œæ— éœ€ç¿»é¡µ")
                return first_page_data

            # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ•°æ®
            all_materials = first_page_data.get('content', {}).get('searchList', [])
            logger.info(f"âœ“ ç¬¬ 1 é¡µ: è·å– {len(all_materials)} æ¡è®°å½•")

            # ä»ç¬¬2é¡µå¼€å§‹ç¿»é¡µ
            for page_num in range(2, total_pages + 1):
                logger.info(f"ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page_num}/{total_pages} é¡µ...")

                # å…ˆå°è¯• API æ–¹å¼
                page_data = self._fetch_page(page_num)

                # å¦‚æœ API å¤±è´¥ï¼Œå°è¯•ç‚¹å‡»æŒ‰é’®ç¿»é¡µ
                if not page_data:
                    logger.warning(f"âš  API ç¿»é¡µå¤±è´¥ï¼Œå°è¯•ç‚¹å‡»æŒ‰é’®ç¿»é¡µ...")
                    if self._click_page_button(page_num):
                        # ç­‰å¾…é¡µé¢åŠ è½½
                        time.sleep(3)
                        # ä»é¡µé¢æå–æ•°æ®
                        page_data = self._extract_current_page_data()

                if page_data:
                    materials = page_data.get('content', {}).get('searchList', [])
                    all_materials.extend(materials)
                    logger.info(f"âœ“ ç¬¬ {page_num} é¡µ: è·å– {len(materials)} æ¡è®°å½• (ç´¯è®¡: {len(all_materials)})")
                    time.sleep(1)
                else:
                    logger.warning(f"âš  ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")

            # æ›´æ–°ç¬¬ä¸€é¡µæ•°æ®ä¸­çš„ searchList
            first_page_data['content']['searchList'] = all_materials

            # ä¿å­˜å®Œæ•´æ•°æ®
            filepath = f"screenshots/{drama_name}_{time_window}.json"
            os.makedirs('screenshots', exist_ok=True)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(first_page_data, f, indent=4, ensure_ascii=False)

            logger.info(f"âœ“ å®Œæ•´æ•°æ®å·²ä¿å­˜: {filepath}")
            logger.info(f"âœ“ æ€»è®¡è·å– {len(all_materials)} æ¡è®°å½•")

            return first_page_data

        except Exception as e:
            logger.error(f"ç¿»é¡µè¿‡ç¨‹å‡ºé”™: {e}")
            return first_page_data

    def _fetch_page(self, page_num: int) -> Optional[Dict]:
        """
        è·å–æŒ‡å®šé¡µç çš„æ•°æ®ï¼ˆé€šè¿‡ç›´æ¥è°ƒç”¨ APIï¼Œä½¿ç”¨ä¿å­˜çš„æœç´¢å‚æ•°ï¼‰

        Args:
            page_num: é¡µç 

        Returns:
            è¯¥é¡µçš„ API å“åº”æ•°æ®
        """
        try:
            logger.info(f"é€šè¿‡ API ç›´æ¥è·å–ç¬¬ {page_num} é¡µæ•°æ®...")

            # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„æœç´¢å‚æ•°
            if not hasattr(self, '_last_search_params') or not self._last_search_params:
                logger.error("æ²¡æœ‰ä¿å­˜çš„æœç´¢å‚æ•°ï¼Œæ— æ³•ç¿»é¡µ")
                return None

            # ä½¿ç”¨ä¿å­˜çš„æœç´¢å‚æ•°ï¼Œåªä¿®æ”¹é¡µç 
            search_params = self._last_search_params.copy()

            # æ ¹æ®å‚æ•°æ ¼å¼å†³å®šå¦‚ä½•ä¿®æ”¹é¡µç 
            if 'pageId' in search_params:
                search_params['pageId'] = str(page_num)
            else:
                search_params['pageNum'] = page_num

            search_params['pageSize'] = '40' if isinstance(list(search_params.values())[0], str) else 40

            # æ„å»ºè¯·æ±‚ä½“ï¼ˆæ”¯æŒ URL ç¼–ç å’Œ JSON ä¸¤ç§æ ¼å¼ï¼‰
            if all(isinstance(v, str) for v in search_params.values()):
                # URL ç¼–ç æ ¼å¼
                body_str = '&'.join([f"{k}={v}" for k, v in search_params.items()])
                content_type = 'application/x-www-form-urlencoded'
                logger.info(f"ä½¿ç”¨ URL ç¼–ç æ ¼å¼: {body_str[:100]}...")
            else:
                # JSON æ ¼å¼
                body_str = json.dumps(search_params)
                content_type = 'application/json'
                logger.info(f"ä½¿ç”¨ JSON æ ¼å¼: {body_str[:100]}...")

            # ä½¿ç”¨ page.evaluate ç›´æ¥è°ƒç”¨ API
            js_fetch_page = f"""
            async () => {{
                try {{
                    const response = await fetch('https://oversea-v2.dataeye.com/api/playlet/creative/searchCreative', {{
                        method: 'POST',
                        headers: {{
                            'Content-Type': '{content_type}',
                        }},
                        body: `{body_str}`
                    }});

                    if (response.ok) {{
                        return await response.json();
                    }}
                    return null;
                }} catch (e) {{
                    console.error('API è°ƒç”¨å¤±è´¥:', e);
                    return null;
                }}
            }}
            """

            json_data = self.page.evaluate(js_fetch_page)

            if json_data:
                logger.info(f"âœ“ æˆåŠŸè·å–ç¬¬ {page_num} é¡µæ•°æ®")
                return json_data
            else:
                logger.warning(f"âš  ç¬¬ {page_num} é¡µæ•°æ®ä¸ºç©º")
                return None

        except Exception as e:
            logger.error(f"è·å–ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
            return None

    def _click_next_page_ui(self, target_page: int) -> bool:
        """
        é€šè¿‡ç‚¹å‡»é¡µé¢æŒ‰é’®ç¿»é¡µï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

        Args:
            target_page: ç›®æ ‡é¡µç 

        Returns:
            æ˜¯å¦ç‚¹å‡»æˆåŠŸ
        """
        try:
            logger.info(f"å°è¯•ç‚¹å‡»ç¿»åˆ°ç¬¬ {target_page} é¡µ...")

            # ç­‰å¾…åˆ†é¡µå…ƒç´ åŠ è½½
            time.sleep(1)

            # å…ˆå°è¯•ç›´æ¥ç‚¹å‡»é¡µç 
            page_num_selectors = [
                f'li[title="{target_page}"]',
                f'.ant-pagination-item-{target_page}',
                f'[class*="pagination"] li:has-text("{target_page}")',
                f'li:has-text("{target_page}")',
            ]

            for selector in page_num_selectors:
                try:
                    page_btn = self.page.query_selector(selector)
                    if page_btn and page_btn.is_visible():
                        logger.info(f"âœ“ æ‰¾åˆ°é¡µç æŒ‰é’®: {selector}")
                        page_btn.click()
                        logger.info(f"âœ“ ç‚¹å‡»é¡µç  {target_page}")
                        time.sleep(2)
                        return True
                except:
                    continue

            logger.warning(f"æœªæ‰¾åˆ°é¡µç  {target_page} çš„æŒ‰é’®")
            return False

        except Exception as e:
            logger.error(f"ç‚¹å‡»ç¿»é¡µå¤±è´¥: {e}")
            return False

    def _is_last_page_ui(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æœ€åä¸€é¡µ

        Returns:
            æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
        """
        try:
            # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
            last_page_indicators = [
                '.ant-pagination-next.ant-pagination-disabled',
                '[class*="pagination"] [class*="next"][disabled]',
                '[class*="pagination"] [class*="next"][class*="disabled"]',
            ]

            for selector in last_page_indicators:
                try:
                    elem = self.page.query_selector(selector)
                    if elem and elem.is_visible():
                        logger.info(f"æ£€æµ‹åˆ°æœ€åä¸€é¡µæ ‡è¯†: {selector}")
                        return True
                except:
                    continue

            return False

        except Exception as e:
            logger.error(f"æ£€æŸ¥æœ€åä¸€é¡µå¤±è´¥: {e}")
            return False

    def _click_time_filter_30d(self) -> bool:
        """
        åˆå¹¶ä¸¤ä¸ªæ—¶é—´çª—å£çš„æ•°æ®

        Args:
            drama_name: çŸ­å‰§åç§°
            data_2y: è¿‘2å¹´çš„ API æ•°æ®
            data_30d: è¿‘30å¤©çš„ API æ•°æ®

        Returns:
            åˆå¹¶åçš„æ•°æ®å­—å…¸
        """
        try:
            logger.info("å¼€å§‹åˆå¹¶ä¸¤ä¸ªæ—¶é—´çª—å£çš„æ•°æ®...")

            # å¦‚æœä¸¤å¹´æ•°æ®ä¸ºç©ºï¼Œè¿”å›å¤±è´¥
            if not data_2y:
                logger.error("è¿‘2å¹´æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                return {
                    "success": False,
                    "drama_name": drama_name,
                    "error": "è¿‘2å¹´æ•°æ®ä¸ºç©º"
                }

            # å¤„ç†è¿‘2å¹´æ•°æ®
            result_2y = self._process_api_response(drama_name, data_2y)
            if not result_2y.get("success"):
                return result_2y

            # å¦‚æœ30å¤©æ•°æ®ä¸ºç©ºï¼Œåªè¿”å›2å¹´æ•°æ®
            if not data_30d:
                logger.warning("è¿‘30å¤©æ•°æ®ä¸ºç©ºï¼Œä»…ä½¿ç”¨è¿‘2å¹´æ•°æ®")
                return result_2y

            # å¤„ç†è¿‘30å¤©æ•°æ®
            result_30d = self._process_api_response(drama_name, data_30d)

            # åˆå¹¶ç»“æœ
            merged_result = {
                "success": True,
                "drama_name": drama_name,
                "window_2y": result_2y.get("window_2y", {}),
                "window_30d": result_30d.get("window_30d", {}) if result_30d.get("success") else result_2y.get("window_30d", {}),
                "total_records_2y": result_2y.get("total_records", 0),
                "total_records_30d": result_30d.get("total_records", 0) if result_30d.get("success") else 0,
            }

            logger.info("âœ“ æ•°æ®åˆå¹¶å®Œæˆ")
            logger.info(f"  - è¿‘2å¹´: {merged_result['window_2y'].get('creative_count', 0)} æ¡ç´ æ")
            logger.info(f"  - è¿‘30å¤©: {merged_result['window_30d'].get('creative_count', 0)} æ¡ç´ æ")

            return merged_result

        except Exception as e:
            logger.error(f"åˆå¹¶æ•°æ®å¤±è´¥: {e}")
            return {
                "success": False,
                "drama_name": drama_name,
                "error": f"åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}"
            }

    def _extract_search_results(self, drama_name: str) -> Dict:
        """æå–æœç´¢ç»“æœæ•°æ® - åŒ…å«æ‰€æœ‰ç»´åº¦"""
        try:
            # ç­‰å¾…æ•°æ®åŠ è½½
            time.sleep(3)

            # æˆªå›¾ä¿å­˜å½“å‰é¡µé¢
            screenshot_path = f"screenshots/search_{drama_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            os.makedirs('screenshots', exist_ok=True)
            self.page.screenshot(path=screenshot_path)
            logger.info(f"å·²ä¿å­˜æˆªå›¾: {screenshot_path}")

            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
            no_data_selectors = [
                'text="æš‚æ— æ•°æ®"',
                'text="No data"',
                '.empty-data',
            ]

            for selector in no_data_selectors:
                try:
                    if self.page.query_selector(selector):
                        logger.warning("æœç´¢ç»“æœä¸ºç©º")
                        return {
                            "success": True,
                            "drama_name": drama_name,
                            "results": [],
                            "message": "æš‚æ— æ•°æ®"
                        }
                except:
                    continue

            # æå–è¡¨æ ¼æ•°æ®
            results = []
            table_rows = self.page.query_selector_all('table tbody tr')

            if table_rows:
                logger.info(f"æ‰¾åˆ° {len(table_rows)} è¡Œè¡¨æ ¼æ•°æ®")
                for row in table_rows:
                    row_data = self._extract_detailed_row_data(row)
                    if row_data:
                        results.append(row_data)
            else:
                logger.warning("æœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®ï¼Œå°è¯•æå–é¡µé¢æ‰€æœ‰æ–‡æœ¬")
                # æå–æ•´ä¸ªé¡µé¢çš„æ–‡æœ¬å†…å®¹ä½œä¸ºå¤‡ä»½
                page_text = self.page.inner_text('body')
                results.append({
                    "type": "page_text",
                    "content": page_text[:5000]  # é™åˆ¶é•¿åº¦
                })

            return {
                "success": True,
                "drama_name": drama_name,
                "results": results,
                "total_count": len(results),
                "screenshot": screenshot_path
            }

        except Exception as e:
            logger.error(f"æå–æœç´¢ç»“æœå¤±è´¥: {e}")
            return {
                "success": False,
                "drama_name": drama_name,
                "error": str(e)
            }

    def _extract_detailed_row_data(self, row) -> Optional[Dict]:
        """ä»è¡¨æ ¼è¡Œæå–è¯¦ç»†æ•°æ® - åŒ…å«æ‰€æœ‰ç»´åº¦"""
        try:
            cells = row.query_selector_all('td')
            if not cells:
                return None

            # æå–æ‰€æœ‰å•å…ƒæ ¼çš„æ–‡æœ¬
            cell_texts = [cell.inner_text().strip() for cell in cells]

            if not cell_texts:
                return None

            # æ„å»ºè¯¦ç»†çš„æ•°æ®ç»“æ„
            row_data = {
                "raw_data": cell_texts,
                "cell_count": len(cell_texts)
            }

            # å°è¯•è§£æå…·ä½“å­—æ®µï¼ˆæ ¹æ®è¡¨æ ¼åˆ—é¡ºåºï¼‰
            # æ³¨æ„ï¼šå®é™…åˆ—é¡ºåºéœ€è¦æ ¹æ®é¡µé¢è°ƒæ•´
            try:
                if len(cell_texts) >= 2:
                    row_data["drama_name"] = cell_texts[0] if cell_texts[0] else None
                    row_data["publisher"] = cell_texts[1] if len(cell_texts) > 1 else None

                # æå–æ•°å€¼å‹æ•°æ®
                for i, text in enumerate(cell_texts):
                    # æ£€æµ‹æ˜¯å¦åŒ…å«æ•°å­—ï¼ˆæ›å…‰é‡ã€ç´ ææ•°ç­‰ï¼‰
                    if any(char.isdigit() for char in text):
                        row_data[f"field_{i}"] = text

            except Exception as parse_error:
                logger.debug(f"è§£æå­—æ®µå¤±è´¥: {parse_error}")

            return row_data

        except Exception as e:
            logger.debug(f"æå–è¡Œæ•°æ®å¤±è´¥: {e}")
            return None

    def _extract_row_data(self, row) -> Optional[Dict]:
        """ä»è¡¨æ ¼è¡Œæå–æ•°æ®ï¼ˆæ—§æ–¹æ³•ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        try:
            cells = row.query_selector_all('td')
            if not cells:
                return None

            # æå–æ‰€æœ‰å•å…ƒæ ¼çš„æ–‡æœ¬
            cell_texts = [cell.inner_text().strip() for cell in cells]

            # è¿”å›åŸå§‹æ•°æ®ï¼Œåç»­å¯ä»¥æ ¹æ®å®é™…è¡¨æ ¼ç»“æ„è°ƒæ•´
            return {
                "raw_data": cell_texts,
                "cell_count": len(cell_texts)
            }

        except Exception as e:
            logger.debug(f"æå–è¡Œæ•°æ®å¤±è´¥: {e}")
            return None

    def _extract_card_data(self, card) -> Optional[Dict]:
        """ä»å¡ç‰‡æå–æ•°æ®"""
        try:
            # æå–å¡ç‰‡å†…çš„æ‰€æœ‰æ–‡æœ¬
            text = card.inner_text().strip()
            if not text:
                return None

            return {
                "raw_text": text
            }

        except Exception as e:
            logger.debug(f"æå–å¡ç‰‡æ•°æ®å¤±è´¥: {e}")
            return None

    def scrape_multiple_dramas(self, drama_list: List[str]) -> List[Dict]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªçŸ­å‰§

        Args:
            drama_list: çŸ­å‰§åç§°åˆ—è¡¨

        Returns:
            æ‰€æœ‰çŸ­å‰§çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []

        for i, drama_name in enumerate(drama_list, 1):
            logger.info(f"\nè¿›åº¦: {i}/{len(drama_list)}")
            result = self.search_drama(drama_name)
            all_results.append(result)

            # æ¯æ¬¡æœç´¢åç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(drama_list):
                wait_time = 3
                logger.info(f"ç­‰å¾… {wait_time} ç§’åç»§ç»­...")
                time.sleep(wait_time)

        return all_results

    def _click_page_button(self, page_num: int) -> bool:
        """
        ç‚¹å‡»é¡µé¢ä¸Šçš„é¡µç æŒ‰é’®

        Args:
            page_num: ç›®æ ‡é¡µç 

        Returns:
            æ˜¯å¦ç‚¹å‡»æˆåŠŸ
        """
        try:
            logger.info(f"å°è¯•ç‚¹å‡»é¡µç  {page_num}...")

            # å°è¯•å¤šç§é€‰æ‹©å™¨
            selectors = [
                f'li[title="{page_num}"]',
                f'[class*="pagination-item-{page_num}"]',
                f'li:has-text("{page_num}")',
            ]

            for selector in selectors:
                try:
                    btn = self.page.query_selector(selector)
                    if btn and btn.is_visible():
                        btn.click()
                        logger.info(f"âœ“ æˆåŠŸç‚¹å‡»é¡µç  {page_num}")
                        return True
                except:
                    continue

            logger.warning(f"æœªæ‰¾åˆ°é¡µç  {page_num} çš„æŒ‰é’®")
            return False

        except Exception as e:
            logger.error(f"ç‚¹å‡»é¡µç å¤±è´¥: {e}")
            return False

    def _extract_current_page_data(self) -> Optional[Dict]:
        """
        ä»å½“å‰é¡µé¢æå–æ•°æ®ï¼ˆé€šè¿‡æ‹¦æˆª APIï¼‰

        Returns:
            å½“å‰é¡µçš„æ•°æ®
        """
        try:
            # ç­‰å¾… API å“åº”
            with self.page.expect_response(
                lambda r: 'searchCreative' in r.url,
                timeout=10000
            ) as response_info:
                pass

            response = response_info.value
            if response.status == 200:
                return response.json()

            return None

        except Exception as e:
            logger.error(f"æå–é¡µé¢æ•°æ®å¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•çˆ¬è™«"""
    logger.info("="*60)
    logger.info("DataEye çŸ­å‰§æ•°æ®é‡‡é›†å™¨")
    logger.info("="*60)

    # æµ‹è¯•å‰§é›†åˆ—è¡¨ - ç”¨æˆ·æŒ‡å®šçš„10éƒ¨çŸ­å‰§
    test_dramas = [
        "å¤©é™èŒå®è€ç¥–ï¼Œå­å­è´¤å­™éƒ½è·ªä¸‹",
        "ç¦»å©šï¼æœ¬å°å§çˆ±çš„èµ·æ”¾å¾—ä¸‹",
        "ç©¿è¿‡è†æ£˜æ‹¥æŠ±ä½ ",
        "ä»–ä¸æ¸¡æˆ‘",
        "æ­¤æƒ…å”¯ä½ å¯æ¶ˆ",
        "æˆ‘æ˜¯å…ƒå©´æœŸï¼å››ä¸ªå§å§ç§ä¸èµ·æˆ‘",
        "å¸¦å´½å«å…¥è±ªé—¨",
        "èœœæ¡ƒä¹Œé¾™",
        "é‡ç”Ÿåæ•´é¡¿å‰å¤«å…¨å®¶",
        "æ–­æ‰‹åŒ»åœ£",
    ]

    try:
        with DataEyeScraper(headless=False) as scraper:
            # ç™»å½•
            logger.info("\næ­¥éª¤ 1: ç™»å½• DataEye å¹³å°")
            if not scraper.login():
                logger.error("ç™»å½•å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
                return

            # æœç´¢çŸ­å‰§
            logger.info("\næ­¥éª¤ 2: å¼€å§‹æœç´¢çŸ­å‰§æ•°æ®")
            results = scraper.scrape_multiple_dramas(test_dramas)

            # ä¿å­˜ç»“æœ
            output_file = f"dataeye_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            logger.info(f"\nâœ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            logger.info(f"âœ“ å…±æœç´¢ {len(results)} ä¸ªçŸ­å‰§")

    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"\nç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
